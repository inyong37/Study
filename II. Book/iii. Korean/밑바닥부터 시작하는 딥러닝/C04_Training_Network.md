# Chapter 4 신경망 학습

## 4.1 데이터에서 학습한다!

### 4.1.1 데이터 주도 학습
기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다. 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다.

이미지에서 특징(feature)를 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 이미지의 특징으 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 많이 사용한다. 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 배표 분류 기법인 SVM, KNN 등으로 학습할 수 있다.

이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계하는 것임에 주의해야 한다. 즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 사람이 적절한 특징을 생각해내야 하는 것이다.

신경망은 이미지를 있는 그대로 학습한다. 신경망은 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습한다.

### Note
딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 한다. 데이터(input)에서 목표한 결과(output)를 사람의 개입 없이 얻는다는 뜻을 담고 있다.

### 4.1.2 훈련 데이터와 시험 데이터

## 4.2 손실 함수

### 4.2.1 평균 제곱 오차
가장 많이 쓰이는 손실 함수는 평균 제곱 오차(mean squared error, MSE)이다.

E = 1/2 * sum((y[k]-t[k])**2) ... [eq 4.1]

여기서 y는 신경망의 출력(신경망이 추정한 값), t는 정답 레이블, k는 데이터의 차원 수를 나타낸다.

```Python
def mean_squared_error(y, t):
  return 0.5 * np.sum((y-t)**2)
```

### 4.2.2 교차 엔트로피 오차
또 다른 손실 함수로서 교차 엔트로피 오차(cross entropy error, CEE)도 자주 이용한다.

E = -sum(t[k]*log(y[k])) ... [eq 4.2]

여기에서 log는 밑이 e인 자연로그(loge)이다. y는 신경망의 출력, t는 정답 레이블이다. 또 t는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다(원-핫 인코딩). 그래서 [eq 4.2]는 정답일 때의 추정(t가 1일 때의 y)의 자연로그를 계산하는 식이된다. 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.

```Python
def cross_entropy_error(y, t):
  return -np.sum(t * np.log(y + delta))
```

아주 작은 값을 더해서 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한다.

### 4.2.3 미니배치 학습

E = -1/N * sum(sum(t[n][k]*log(y[n][k]))) ... [eq 4.3]

데이터가 N개라면, t[n][k]는 n번째 데이터의 k번째 값을 의미한다. y[n][k]는 신경망의 출력, t[n][k]는 정답 레이블이다. 데이터 하나에 대한 손실 함수인 [eq 4.2]를 단순히 N개의 데이터로 확장했을 뿐이다. 다만, 마지막에 N으로 나누어 정규화하고 있다. N으로 나눔으로써 평균 손실 함수를 구하는 것이다. 이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다.

데이터 일부를 추려 전체의 근사치로 이용할 수 있다. 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행한다. 이 일부를 미니 배치(mini-batch)라 한다. 가령 60000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 것이다. 이러한 학습 방법을 미니배치 학습이라고 한다.

### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기
```Python
def cross_entropy_error(y, k):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)
  batch_size = y.shape[0]
  return -np.sum(t * np.log(y)) / batch_size
```

y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape 함수로 데이터의 형상을 바꿔준다. 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다.

정답 레이블이 원-핫 인코딩이 아니라 2나 7 등의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다음과 같이 구현한다.

```Python
def cross_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)
  batch_size = y.shape[0]
  return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size
```

이 구현에서는 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다. 그래서 원-핫 인코딩 시 t*np.log(y)였던 부분을 레이블 표현일 때는 np.log(y[np.arange(batch_size), t])로 구현한다.



## 4.3 수치 미분

## 4.4 기울기

## 4.5 학습 알고리즘 구현하기

## 4.6 정리
- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.
- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.
- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.
- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.
- 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.
- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.
- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 오차역전파법은 기울기를 고속으로 구할 수 있다.
