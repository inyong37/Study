# Chapter 3. 현대 신경망

## 3.1. 기술 요구사항

## 3.2. 합성곱 신경망의 발견

### 3.2.1. 다차원 데이터를 위한 신경망

### 완전 연결 네트워크의 문제점

이미지를 처리할 때 기초 네트웤가 갖는 두 가지 주요 단점은 다음과 같다.

- 매개변수의 폭발적인 증가
- 공간 추론의 부족

### 매개변수의 폭발적인 증가

이미지는 H * W * D 값으로 이루어진다. H는 이미지 높이, W는 이미지 너비, D는 이미지의 깊이 혹은 채널 개수(RGB 이미지의 경우 D=3)이다. 예를 들어, MNIST 이미지의 경우 작은 단일 채널 이미지도 28 * 28 * 1 = 784개의 값으로 이루어진 입력 벡터를 가지고, 첫 번째의 계층이 64개의 노드로 이루어져 있으면 (784, 64) 형상의 weight vector를 가진다. 이 때 이 변수만을 최적화해야 할 매개변수의 값은 784 * 64 = 50176개이다. RGB 이미지가 커지거나 네트워크가 깊어질수록 '이 매개변수 개수는 매우 급격히 증가한다.'

### 공간 추론의 부족

이 네트워크의 뉴런이 어떤 구분 없이 이전 계층의 모든 값을 받기 때문에('뉴런이 모두 연결되어 있다.) 이 신경망은 '거리/공간성'이 없다. 데이터의 공간 관계를 잃는다. 이미지 같은 다차원 데이터는 밀접 계층에 칼럼 벡터로 전달될 수 있기 때문에 그 연산은 데이터 차원이나 입력값의 위치를 고려하지 않는다. 더 정확하게는 모든 픽셀 값이 계층별로 '원래 위치와 상관없이' 결합되므로 픽셀 사이의 근접성 개념이 완전 연결(FC, Fully-Connected) 계층에서 손실된다.

:bulb: 평면화(Flatten)를 해도 밀집 계층의 행위가 바뀌지 않으므로 밀집 계층의 계산과 매개변수 표현을 단순화하기 위해 밀집 계층에 전달하기 전에 다차원 입력을 '1차원으로 변환'하는 것(즉 컬럽 벡토로 형상을 바꾸는 것)이 보편적이다.

### CNN 도입

CNN은 다차원 데이터를 처리할 수 있다. 이미지의 경우, CNN은 3차원 데이터(높이 * 너비 * 깊이)를 입력으로 취하고 뉴런을 그와 비슷한 볼륨으로 정렬한다. CNN의 각 뉴런은 이전 계층에서 이웃한 영역에 속한 일부 요소에만 접근한다. 이 영역을 뉴런의 '수용 영역' 또는 '필터 크기'라 한다. 뉴런을 이전 계층의 이웃한 뉴런과만 연결함으로써 CNN은 훈련시킬 '매개변수 개수를 급격히 줄일'뿐 아니라 '이미지 특징의 위치 정보를 보존'한다.

### 3.2.2. CNN 작업

이 아키텍처 패러다임으로 새로운 유형의 계층도 도입해 '다차원성'과 '지역적 연결성'을 효율적으로 활용한다.

### 합성곱 계층

CNN이라는 이름은 그 아키텍처의 핵심에 해당하는 '합성곱 계층'에서 비롯됐다. 이 계층에서는 동일한 출력 채널에 연결된 모든 뉴런이 똑같은 가중치와 편향값을 공유함으로써 매개변수의 개수를 더 줄일 수 있다.

### 개념

가중치와 편향값을 공유하는 특정 뉴런은 '공간적으로 제한된 연결성'을 통해 전체 입력 행렬에서 슬라이딩하는 단일 뉴런으로 생각할 수도 있다. 각 단계에서 이 뉴런은 현재 슬라이딩하고 있는 입력 볼륨(H * W * D)의 일부 영역에만 공간적으로 연결된다. 필터 크기가 (k_H, k_W)인 뉴런에 대해 이 제한된 입력 차원 k_H * k_W * D가 주어지면 이 뉴런은 첫 번째 장에서 모델링했던 뉴런처럼 동작한다. 즉 합계에 활성화 함수(선형 또는 비선형 함수)를 적용하기 전에 입력 값(k_H * k_W * D개의 값)을 선형으로 결합한다. 수학적으로 (i, j) 위치에서 시작한 입력 패치에 대한 뉴런의 응답 z_ij는 다음과 같이 표현할 수 있다.

z_ij = delta * (b + sum(l=0 ~ k_H - 1 (sum (m=0 ~ k_W - 1 sum(n=0 ~ D - 1(w_(l,m,n) * x_(i+l,j+m,n) ))))

### 속성

### 초매개변수

### 텐서플로/케라스 메서드

### 풀링 계층

### 개념 및 초매개변수

### 텐서플로/케라스 메서드

### 완전 연결 계층

### CNN에서의 사용법

이 계층은 일반적으로 네트워크의 마지막 계층에서 예를 들어 다차원 특징을 1차원 분류 벡터로 변환하기 위해 사용된다.

### 텐서플로/케라스 메서드

다차원 텐서를 밀집 계층에 전달하기 전에 '평면화(Flattening)'해야 한다는 점을 기억하자.

### 3.2.3. 유효 수용 영역

### 정의

### 공식

### 3.2.4. 텐서플로로 CNN 구현하기

### 첫 CNN 구현

### LeNet-5 아키텍처

### 텐서플로와 케라스 구현

### MNIST에 적용

## 3.3. 훈련 프로세스 개선

### 3.3.1. 현대 네트워크 최적화 기법

### 경사 하강법의 까다로운 점

### 훈련 속도와 트레이드오프

### 준최적 극솟값(Suboptimal local minima)

### 이기종 매개변수를 위한 단일 초매개변수

전통적인 경사 하강법에서는 동일한 학습률이 네트워크의 모든 매개변수를 업데이트하는 데 사용된다. 그렇지만 이 모든 변수가 변화에 동일한 민감도를 갖지 않으며, 반복할 때마다 모든 변수가 손실에 영향을 주지 않는다. 결정적인 매개변수를 좀 더 신중하게 업데이트하기 위해 학습률을 다르게 적용하고(예를 들어, 매개변수 하위집합 단위로), 네트워크 예측에 충분히 기여하지 않는 매개변수는 좀 더 과감하게 업데이트하는 것이 이로울 수 있다.

### 고급 최적화 기법

### 모멘텀 알고리즘

### Ada 군

Adagrad, Adadelta, Adam 등은 각 뉴런의 민감도 및 활성화 빈도에 따라 학습률을 조정하는 아이디어에 몇 가지 반복과 변형을 준 알고리즘이다.

### 3.3.2. 정규화 기법

### 조기 중단

### L1, L2 정규화

### 원리

### 텐서플로와 케라스 구현

### 드롭아웃

### 정의

### 텐서플로 및 케라스 메서드

### 배치 정규화

### 정의

#### 텐서플로 및 케라스 메서드

## 3.4. 요약

- CNN은 현대 컴퓨터 비전과 머신러닝에서 가장 중요한 위치를 차지하기 때문에 CNN이 어떻게 동작하고 어떤 종류의 계층으로 구성되는지 이해하는 것이 중요함
- 텐서플로와 케라스는 그러한 네트워크를 효율적으로 구성할 수 있는 쉬운 인터페이스를 제공함
- 모든 애플리케이션에서 염두에 둬야 할 중요한 점인 훈련된 모델의 성능과 견고함을 개선하기 위해 몇 가지 고급 최적화 기법과 정규화 기법(다양한 최적화기, L1/L2 정규화, 드롭아웃, 배치 정규화 등)을 구현함

## 3.5. 질문

## 3.6. 참고 문헌
