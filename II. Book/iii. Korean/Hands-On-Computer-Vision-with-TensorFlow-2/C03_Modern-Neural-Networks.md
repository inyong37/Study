# Chapter 3. 현대 신경망

## 3.1. 기술 요구사항

## 3.2. 합성곱 신경망의 발견

### 3.2.1. 다차원 데이터를 위한 신경망

### 완전 연결 네트워크의 문제점

이미지를 처리할 때 기초 네트웤가 갖는 두 가지 주요 단점은 다음과 같다.

- 매개변수의 폭발적인 증가
- 공간 추론의 부족

### 매개변수의 폭발적인 증가

이미지는 H * W * D 값으로 이루어진다. H는 이미지 높이, W는 이미지 너비, D는 이미지의 깊이 혹은 채널 개수(RGB 이미지의 경우 D=3)이다. 예를 들어, MNIST 이미지의 경우 작은 단일 채널 이미지도 28 * 28 * 1 = 784개의 값으로 이루어진 입력 벡터를 가지고, 첫 번째의 계층이 64개의 노드로 이루어져 있으면 (784, 64) 형상의 weight vector를 가진다. 이 때 이 변수만을 최적화해야 할 매개변수의 값은 784 * 64 = 50176개이다. RGB 이미지가 커지거나 네트워크가 깊어질수록 '이 매개변수 개수는 매우 급격히 증가한다.'

### 공간 추론의 부족

이 네트워크의 뉴런이 어떤 구분 없이 이전 계층의 모든 값을 받기 때문에('뉴런이 모두 연결되어 있다.) 이 신경망은 '거리/공간성'이 없다. 데이터의 공간 관계를 잃는다. 이미지 같은 다차원 데이터는 밀접 계층에 칼럼 벡터로 전달될 수 있기 때문에 그 연산은 데이터 차원이나 입력값의 위치를 고려하지 않는다. 더 정확하게는 모든 픽셀 값이 계층별로 '원래 위치와 상관없이' 결합되므로 픽셀 사이의 근접성 개념이 완전 연결(FC, fully-connected) 계층에서 손실된다.

:bulb: 평면화(flatten)를 해도 밀집 계층의 행위가 바뀌지 않으므로 밀집 계층의 계산과 매개변수 표현을 단순화하기 위해 밀집 계층에 전달하기 전에 다차원 입력을 '1차원으로 변환'하는 것(즉 컬럼 벡토로 형상을 바꾸는 것)이 보편적이다.

### CNN 도입

CNN은 다차원 데이터를 처리할 수 있다. 이미지의 경우, CNN은 '3차원 데이터(높이 * 너비 * 깊이)'를 입력으로 취하고 뉴런을 그와 비슷한 볼륨으로 정렬한다. 뉴런이 이전 계층의 모든 요소에 연결된 완전 연결 네트워크와 달리 CNN의 각 뉴런은 이전 계층에서 이웃한 영역에 속한 일부 요소에만 접근한다. 이 영역을 뉴런의 수용 영역(또는 '필터 크기')이라고 한다. 뉴런을 이전 계층의 이웃한 뉴런과만 연결함으로써 CNN은 훈련시킬 '매개변수 개수를 급격히 줄일'뿐 아니라 '이미지 특징의 위치 정보를 보존'한다.

### 3.2.2. CNN 작업

이 아키텍처 패러다임으로 새로운 유형의 계층도 도입해 '다차원성'과 '지역적 연결성'을 효율적으로 활용한다.

### 합성곱 계층

CNN이라는 이름은 그 아키텍처의 핵심에 해당하는 '합성곱 계층'에서 비롯됐다. 이 계층에서는 동일한 출력 채널에 연결된 모든 뉴런이 똑같은 가중치와 편향값을 공유함으로써 매개변수의 개수를 더 줄일 수 있다.

### 개념

가중치와 편향값을 공유하는 특정 뉴런은 '공간적으로 제한된 연결성'을 통해 전체 입력 행렬에서 슬라이딩하는 단일 뉴런으로 생각할 수도 있다. 각 단계에서 이 뉴런은 현재 슬라이딩하고 있는 입력 볼륨(H * W * D)의 일부 영역에만 공간적으로 연결된다. 필터 크기가 (k_H, k_W)인 뉴런에 대해 이 제한된 입력 차원 k_H * k_W * D가 주어지면 이 뉴런은 첫 번째 장에서 모델링했던 뉴런처럼 동작한다. 즉 합계에 활성화 함수(선형 또는 비선형 함수)를 적용하기 전에 입력 값(k_H * k_W * D개의 값)을 선형으로 결합한다. 수학적으로 (i, j) 위치에서 시작한 입력 패치에 대한 뉴런의 응답 z_ij는 다음과 같이 표현할 수 있다.

z_ij = delta * (b + sum(l=0 ~ k_H - 1 (sum (m=0 ~ k_W - 1 sum(n=0 ~ D - 1(w_(l,m,n) * x_(i+l,j+m,n) ))))

:bulb: 실제로 대부분 정사각형 필터를 사용한다. 이는 이 필터의 크기가 (k, k)이고, k=k_H=k_W라는 뜻이다.

합성곱 계층에는 여전히 N개의 다른 뉴런 집합(즉 같은 매개변수를 공유하는 N개의 뉴런 집합)이 있으므로, 그에 대한 응답 맵이 함께 쌓여서 형상이 H_o * W_o * N인 출력 텐서가 된다.

완전 연결 계층에서 행렬 곱셈을 적용한 것과 동일한 방식으로 여기에서 합성곱 연산을 사용해 모든 응답 맵을 한번에 계산할 수 있다(그래서 이 계층의 이름이 합성곱 계층이다).

:bulb: 머신러닝 분야에서는 '합성곱'이라는 말을 보편적으로 쓰지만, 이 연산을 수학적 용어로 적절하게 표현하자면 실제로 '교차 상관 관계(cross-correlation)'이라고 볼 수 있다.

### 속성

N개의 다양한 뉴런의 집합을 갖는 합성곱 계층은 형상이 D * k * k(필터가 정사각형인 경우)인 N개의 가중치 행렬(필터 또는 커널이라고도 함)과 N개의 편향값으로 정의된다. 따라서 이 계층에서 훈련시킬 값은 N * (D * K^2 + 1)개 뿐이다. 반면 완전 연결 계층이라면 유사한 입력과 출력 차원을 가질 때 (H * W * D) * (H_o * W_o * N)개의 매개변수가 필요하다. 완전 연결 계층에서 매개변수 개수는 데이터 차원에 영향을 받는 반면 합성곱 계층에서는 데이터 차원이 매개변수 개수에 영향을 주지 않는다.

합성곱 계층은 어떤 이미지에도 '그 차원 수와 상관없이' 적용될 수 있다는 점이다. 완전 연결 계층을 가진 네트워크와 달리 순수 합성곱 네트워크에서는 입력 크기가 다양하더라도 별도의 조정이나 재 훈련 과정을 거칠 필요가 없다.

:bulb: CNN을 다양한 크기의 이미지에 적용하는 경우, 입력 배치를 샘플링할 때 주의할 점이 있다. 실제로 이미지 하위 집합은 모두 동일한 차원을 가질 때만 함께 쌓여서 일반 배치 텐서가 될 수 있다. 따라서 실제로 이미지 배치를 나누기 (대체로 훈련 단계에 수행) 전에 정렬하거나 단순히 각 이미지를 개별로 처리(일반적으로 테스트 단계에서 수행)해야 한다. 하지만 데이터 처리와 네트워크 작업을 모두 단순화하기 위해 일반적으로는 이미지를 전처리해서 이미지가 모두 동일한 크기를 갖게 한다(크기를 조정하거나 자르는 작업을 통해).

### 초매개변수

합성곱 계층은 우선 필터 개수 N, 입력 깊이 D(즉 입력 채널의 개수), 필터/커널 크기 (k_H, k_W)로 정의된다. 일반적으로 정사각형 필터를 사용하기 때문에 보통 필터 크기는 간단하게 k로 정의된다.

필터가 움직이는 '보폭(stride)'을 다양하게 적용할 수 있다. 따라서 보폭이라는 초매개변수는 필터가 움직일 때 이미지 패치와 필터 사이의 내적을 위치마다 계산할지(stride=1), s 위치마다 계산할 지 (stride=s) 정의한다. 보폭이 커지면 결과 특징 맵은 희소해진다. 

이미지는 합성곱을 적용하기 전에 '0으로 패딩'될 수도 있다. 즉, 이미지 크기를 원본 콘텐츠 주변에 0으로 된 행과 열을 추가해 인위적으로 키울 수 있다. 패딩은 필터가 이미지를 차지할 수 있는 위치의 수를 증가시킨다. 따라서 적용할 패딩 값(즉, 입력 주변에 추가할 빈 행과 열의 개수)을 지정할 수 있다.

:bulb: 문자 k는 보통 필터/커널 크기(k는 커널 kernel의 앞 글자를 따온 것이다)를 뜻한다. 마찬가지로 s는 보통 보폭(stride), p는 패딩(padding)을 뜻한다. 필터 크기에 대해 말하자면, 일반적으로 수평 및 수직 보폭(s=s_H=s_W)은 물론 수평 및 수직 패딩에도 동일한 값이 사용된다. 그렇지만 일부 특정 사례에서는 다른 값을 가질 수도 있다.

이 모든 매개변수는 계층 연산 뿐만 아니라 그 계층의 출력 형상에도 영향을 준다. 지금까지 여기서는 이 형상을 (H_O, W_O, N)으로 정의했고, 여기에서 'H_O와 W_O'는 뉴런이 입력에서 수직/수평적으로 움직일 수 있는 횟수'를 말한다. 

H_O = (H - k + 2p) / s + 1

W_O = (W - k + 2p) / s + 1

크기가 k인 필터는 크기가 H * W인 이미지에서 수직으로는 최대 H-k+1개의 위치를 취하고 수평으로 W-k+1개의 위치를 취할 수 있다. 게다가 이 이미지의 모든 변에 p만큼 패딩하면 이 위치 개수는 H-k+2p+1 (W-k+2p+1)까지 커진다. 마지막으로 보폭 s를 증가시키는 것은 s 중 하나의 위치만 고려한다는 뜻이며, 이는 위 공식에서 나눗셈을 설명한다(정수 나눗셈임을 주목하자).

이 초매개변수로 계층의 출력 크기를 쉽게 제어할 수 있다. 이것은 객체 분할 같은 애플리케이션 즉, 출력 분할 마스크가 입력 이미지와 동일한 크기를 갖기를 원할 때 특히 편리하다.

### 텐서플로/케라스 메서드

### 풀링 계층

### 개념 및 초매개변수

이 풀링 계층에는 '훈련 가능한 매개변수가 없어서' 다소 특이하다. 각 뉴런은 자기 '윈도우'(수용 영역)의 값을 취하고 사전에 정의된 함수로 계산한 하나의 출력을 반환한다.

### 텐서플로/케라스 메서드

### 완전 연결 계층

### CNN에서의 사용법

이 계층은 일반적으로 네트워크의 마지막 계층에서 예를 들어 다차원 특징을 1차원 분류 벡터로 변환하기 위해 사용된다.

### 텐서플로/케라스 메서드

다차원 텐서를 밀집 계층에 전달하기 전에 '평면화(Flattening)'해야 한다는 점을 기억하자.

### 3.2.3. 유효 수용 영역

### 정의

### 공식

### 3.2.4. 텐서플로로 CNN 구현하기

### 첫 CNN 구현

### LeNet-5 아키텍처

### 텐서플로와 케라스 구현

### MNIST에 적용

## 3.3. 훈련 프로세스 개선

### 3.3.1. 현대 네트워크 최적화 기법

### 경사 하강법의 까다로운 점

### 훈련 속도와 트레이드오프

### 준최적 극솟값(Suboptimal local minima)

### 이기종 매개변수를 위한 단일 초매개변수

전통적인 경사 하강법에서는 동일한 학습률이 네트워크의 모든 매개변수를 업데이트하는 데 사용된다. 그렇지만 이 모든 변수가 변화에 동일한 민감도를 갖지 않으며, 반복할 때마다 모든 변수가 손실에 영향을 주지 않는다. 결정적인 매개변수를 좀 더 신중하게 업데이트하기 위해 학습률을 다르게 적용하고(예를 들어, 매개변수 하위집합 단위로), 네트워크 예측에 충분히 기여하지 않는 매개변수는 좀 더 과감하게 업데이트하는 것이 이로울 수 있다.

### 고급 최적화 기법

### 모멘텀 알고리즘

### Ada 군

Adagrad, Adadelta, Adam 등은 각 뉴런의 민감도 및 활성화 빈도에 따라 학습률을 조정하는 아이디어에 몇 가지 반복과 변형을 준 알고리즘이다.

### 3.3.2. 정규화 기법

### 조기 중단

### L1, L2 정규화

### 원리

### 텐서플로와 케라스 구현

### 드롭아웃

### 정의

### 텐서플로 및 케라스 메서드

### 배치 정규화

### 정의

#### 텐서플로 및 케라스 메서드

## 3.4. 요약

- CNN은 현대 컴퓨터 비전과 머신러닝에서 가장 중요한 위치를 차지하기 때문에 CNN이 어떻게 동작하고 어떤 종류의 계층으로 구성되는지 이해하는 것이 중요함
- 텐서플로와 케라스는 그러한 네트워크를 효율적으로 구성할 수 있는 쉬운 인터페이스를 제공함
- 모든 애플리케이션에서 염두에 둬야 할 중요한 점인 훈련된 모델의 성능과 견고함을 개선하기 위해 몇 가지 고급 최적화 기법과 정규화 기법(다양한 최적화기, L1/L2 정규화, 드롭아웃, 배치 정규화 등)을 구현함

## 3.5. 질문

## 3.6. 참고 문헌
