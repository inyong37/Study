# Chapter 4. 유력한 분류 도구

- VGG, Inception, ResNet 같은 아키텍처가 컴퓨터 비전 분야에 가져온 것
- 이 솔루션들을 분류 작업을 위해 재구현하거나 직접 재활용할 방법
- 전이학습의 정의와 훈련된 네트워크를 다른 용도에 맞게 효율적으로 바꾸는 방법

## 4.1 기술 요구사항

## 4.2 고급 CNN 아키텍처의 이해

### 4.2.1 VGG, 표준 CNN 아키텍처

#### VGG 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - CNN 아키텍처 표준화

##### 규모가 큰 합성곱을 여러 작은 합성곱으로 대체

- 매개변수 개수를 줄인다: 실제로 11 * 11 합성곱 계층에 N개의 필터를 적용한다는 것은 커널만을 위해 훈련시켜야 할 값이 11 * 11 * D * N=121DN개라는 것을 뜻한다. 반면 5개의 3 * 3 합성곱에는 커널을 위한 가중치가 총 1 * (3 * 3 * D * N) + 4 (3 * 3 * N * N)=9DN + 36N^2개가 있다. N < 3.6D 이기만 하면 매개변수 개수가 더 작다는 뜻이다. 예를 들어 N = 2D이면 매개변수 개수는 242D^2에서 153D^2개로 떨어진다. 이렇게 되면 네트워크를 최적화하기 쉬워지고 훨씬 가벼워진다.
- 비선형을 증가시킨다: 합성곱 계층 개수가 커지면, 그리고 각 합성곱 계층 다음에 ReLU 같은 '비선형' 활성화 함수가 오면 네트워크가 복잡한 특징을 학습할 수 있는 능력이 증대된다(즉, 더 많은 비선형 연산을 결합함으로써).

##### 특징 맵 깊이를 증가

직관을 기반으로 각 합성곱 블록에 대한 특징 맵의 깊이를 두 배로 늘렸다(첫 번째 합성곱 다음에 64에서 512로). 각 집합 다음에는 윈도우 크기가 2 * 2이고 보폭이 2인 최대-풀링 계층이 나오므로 깊이가 두 배로 늘고 공간 차원은 반으로 줄게 된다.

##### 척도 변경을 통한 데이터 보강

##### 완전 연결 계층을 합성곱 계층으로 대체

전통적인 VGG 아키텍처는 마지막에 여러 개의 오나전 연결 계층이 오지만(AlexNet처럼), 이 논문에서는 다른 방식을 제안한다. 여기서 제안한 아키텍처에서는 밀집 계층을 합성곱 계층으로 대체한다. 크기가 좀 더 큰 커널(7 * 7과 3 * 3)을 적용한 첫 번째 합성곱 세트는 특징 맵의 공간 크기를 1 * 1로 줄이고(그 전에 패딩을 적용하지 않았다면) 특징 맵의 깊이를 4096으로 늘린다. 마지막으로 1 * 1 합성곱 계층이 예측해야 할 클래스 개수만큼의 필터(즉, ImageNet의 경우 N=1000)와 함께 사용된다. 그 결과 얻게 된 1 * 1 * N 벡터는 softmax 함수로 정규화된 다음 평면화되어 최종 클래스 예층으로 출력된다(벡터의 각 값은 예측된 클래스 확률을 나타낸다).

밀집 계층을 두지 않는 이러한 네트워크를 완전 합성곱 계층(Fully Connected Network, FCN)이라고 한다. FCN은 사전에 이미지를 자르지 않고도 다양한 크기의 이미지에 적용될 수 있다.

:bulb: 흥미롭게도 ILSVRC에서 가장 높은 정확도를 얻기 위해 저자는 일반 합성곱 신경망과 FCN을 모두 훈련시켜 사용하고 두 결과의 평균을 구해 최종 예측을 얻었다. 이러한 기법을 모델 평균법(Model Averaging)이라고 하고 실제 운영 환경에서 자주 사용한다.

#### 텐서플로와 케라스로 구현하기

##### 텐서플로 모델

##### 케라스 모델

:bulb: 케라스 용어에서 최상단 계층은 마지막에 연이어 놓인 밀집 계층을 말한다. 따라서 include_top=False로 설정하면 VGG 밀집 계층을 제외한다는 뜻이고 이 때 네트워크 출력은 마지막 합성곱/최대-풀링 블록의 특징 맵이 된다. 이는 분류 작업뿐 아니라 의미 있는 특징을 추출하기 위해(보다 발전된 작업에 적용할 수 있는) 사전에 훈련된 VGG 네트워크를 재사용하고자 할 경우 유용하다. pooling 함수 매개변수는 이러한 경우(즉 include_top=False일 때) 특징 맵을 반환하기 전에 적용할 선택적인 연산을 지정하기 위해 사용될 수 있다(평균-풀링 또는 최대-풀링을 적용한다면 pooling='avg' 또는 pooling='max').

### 4.2.2. GoogLeNet, Inception 모듈

#### GoogLeNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 규모가 큰 블록과 병목을 보편화

##### 인셉션 모듈로 다양한 세부 특징 잡아내기

##### 병목 계층으로 1 * 1 합성곱 계층을 사용

:bulb: 이 장에서는 ILSVRC 2014에 제출된 GoogLeNet을 설명한다. 일반적으로 사용하는 명칭은 인셉션 V1으로, 이 아키텍처는 그 이후로 저자들에 의해 개선됐다. 인셉션 V2와 인셉션 V3에는 5 * 5와 7 * 7 합성곱 계층을 그보다 작은 합성곱 계층으로 대체하고(VGG에서와 마찬가지로) 정보 손실을 줄이기 위해 병목 계층의 초매개변수를 개선하거나 'BatchNorm' 계층을 추가하는 등 몇 가지 개선사항이 포함돼 있다. 

##### 완전 연결 계층 대신 풀링 계층 사용

##### 중간 손실로 경사 소실 문제 해결하기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 Inception 모듈 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

### 4.2.3. ResNet - 잔차 네트워크

#### ResNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 정보를 더 깊은 계층으로 전방 전달

##### 매핑 대신 잔차 함수 추정하기

##### '극단적으로 깊이' 들어가기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 잔차 블록 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

## :pencil: 4.3. 전이학습 활용

### 4.3.1. 개요

#### 정의

##### 인간으로부터 영감 얻기

기존에 보유한 지식을 기반으로 복잡한 작업에 숙달하거나 비슷한 행동에 이미 가지고 있는 기술을 바꿔 적용하는 능력은 인간 지능에서 가장 중요한 부분이다. 머신러닝 연구원들은 이 능력을 복제하는 꿈을 꾸고 있다.

##### 동기

CNN은 특정 특징을 추출해 해석하도록 훈련됐기 때문에 특징 분포가 바뀌면 그 성능은 떨어지게 된다. 따라서 네트워크를 새로운 작어베 적용하려면 어느 정도의 변환 작업이 필요하다.

그 해법에 대해 수십년간 연구됐다. 1998년 세바스찬 스런(Sebastian Thrun)과 로리엔 프랫(Lorien Pratt)은 이 주제에 기초한 널리 알려진 연구를 엮어 Learning to Learn이라는 책을 편집했다. 더 최근에는 이안 굿펠로(Ian Goodfellow)와 요슈아 벤지오(Yoshua Bengio), 애런 쿠빌(Aaron Courille)이 심층 학습(Deep Learning)에서 전이학습을 다음과 같이 정의했다. *어떤 설정(예를 들어, 분포 P_1)에서 학습된 내용이 다른 설정(분포 P_2라고 하자)에서 일반화를 개선하기 위해 활용되는 상황*

:bulb: 머신러닝에서 작업(task)는 주어진 입력(예를 들어, 스마트폰으로 찍은 사진)과 예상 출력(예를 들어, 특정 클래스 집합에 대한 예측 결과)에 의해 정의된다. 예를 들어, ImageNet에서 분류하거나 탐지하는 작업은 입력 이미지는 동일하지만 출력은 다르기 때문에 서로 다른 두 개의 작업으로 봐야 한다. 경우에 따라 알고리즘은 유사한 작업(예를 들어, 보행자 탐지)을 목표로 하지만 다른 데이터셋(예를 들어, 다양한 위치의 CCTV 이미지나 다양한 품질의 카메라로 찍은 이미지)를 사용하기도 한다. 따라서 이 기법은 다양한 도메인(즉, 데이터 분포)에서 훈련된다. 전이학습은 이미 갖춘 지식을 하나의 작업에서 다른 작업으로 또는 한 도메인에서 다른 도메인으로 적용하는 것을 목표로 한다. 이 중 한 도메인에서 다른 도메인으로 적용하는 형태의 전이학습을 도메인 적응(Domain Adaption)이라고 한다.

전이학습은 새로운 작업을 적절하게 학습하기에 충분한 데이터가 확보되지 않았을 때(즉, 분포를 추정하기에 이미지 샘플이 충분하지 않을 때) 매력적이다. 특히 지도 학습을 위해 레이블이 붙은 데이터셋을 수집하는 일은 불가능하지 않더라도 지루하고 따분한 작업이다.

:bulb: ImageNet(최근에는 COCO)은 수많은 카테고리에서 나온 주석이 달른 이미지를 수백만 개 포함하고 있어 특히 풍부한 데이터셋이다. 이 데이터셋에서 훈련된 CNN은 시각 인식 작업에서 상당한 전문 역량을 습득했다고 가정하므로 케라스과 텐서플로 허브에서도 이 데이터셋에서 이미 훈련된 표준 모델(인셉션, ResNet-50 등)을 제공한다. 지식전이할 모델을 찾는 사람들은 일반적으로 이 표준 모델을 사용한다.

##### CNN 지식 전이

인공 신경망이 사람의 뇌보다 나은 점 중 하나는 저장과 복제가 쉽다는 점이다. CNN이 갖고 있는 전문지식은 훈련 이후 매개변수가 취한 값, 즉 쉽게 복원되고 유사한 네트워크에 전이될 수 있는 값이 전부다.

CNN을 위한 전이학습은 주로 다른 작업을 위한 새로운 모델을 인스턴스화하기 위해 풍부한 데이터셋에서 훈련된 성능 좋은 네트워크의 아키텍처 전체 혹은 일부와 가중치를 재사용하는 것으로 구성된다. 새로운 모델은 이 조건에 따라 인스턴스화한 다음 '미세 조정'될 수 있다. 즉 새로운 작업/도메인에 대해 활용할 수 있는 데이터에서 더 훈련될 수 있다.

네트워크의 첫 번째 계층은 저차원 특징(선, 테두리, 색 변화 등)을 추출하는 경향이 있지만 마지막 합성곱 계층은 더 복잡한 개념(특정 형태나 패턴 같이)에 반응한다. 분류 작업을 위해 마지막 풀링 또는 완전 연결 계층에서 클래스를 예측하기 위해 이 고차원 특징 맵(병목 특징, Bottleneck Feature)을 처리한다.

이 일반적인 구성과 관련 관측을 통해 다양한 전이학습 전략이 도출됐다. 마지막 예측 계층을 제거한 사전 훈련된 CNN은 효율적으로 '특징을 추출'하는 용도로 사용되기 시작했다. 새로운 작업이 이 추출기가 훈련된 목적과 충분히 비슷한 경우 적절한 특징을 출력하기 위해 바로 사용될 수 있다(이 목적에 정확히 부합하는 '이미지 특정 벡터' 모델을 텐서플로 허브에서 찾아볼 수 있다). 그런 다음 이 특징은 작업과 관련된 예측을 출력하도록 훈련된 한두 개의 새로 추가된 밀집 계층에서 처리될 수 있다. 추출된 특징의 품질을 유지하기 위해 대체로 이 훈련 단계 동안 특징 추출기의 계층을 '고정'시킨다. 즉, 그 계층의 매개변수는 경사 하강이 일어나는 동안 업데이트되지 않는다. 이와 다르게 작업/도메인이 유사하지 않은 경우 특징 추출기의 마지막 계층 중 일부 또는 전체를 '미세 조정'한다. 즉, 이 계층들을 작업 데이터에서 새로운 예측 계층과 함께 훈련시킨다.

#### 활용 사례

사전 훈련된 모델 중 어떤 모델을 재사용할지, 어느 계층을 고정시키고 어느 계층을 미세 조정할 지는 목표한 작업과 모델이 이미 훈련한 작업과 얼마나 비슷한지, 그리고 새로운 애플리케이션을 위한 훈련 샘플이 얼마나 풍부한지에 따라 다르다.

##### :pencil2: 제한적 훈련 데이터로 유사한 작업 수행

전이학습은 특정 작업을 해결하고 싶고 선은 좋은 모델을 제대로 훈련시킬 만큼 훈련 샘플이 충분하지 않지만 그보다 크고 유사한 훈련 데이터셋에 접근할 수 있을 때 특히 유용하다.

이 모델은 이 큰 데이터셋에서 수렴할 때까지 사전 훈련시킬 수 있다(또는 가능하고 적절하다면, 제공되는 사전 훈련된 모델을 가져올 수도 있다). 그런 다음 마지막 계층을 제거하고(목표한 작업이 다른 경우, 즉 그 출력이 사전 훈련의 목표 작업의 출력과 다른 경우) 목표한 작업에 맞춰 조정한 계층으로 교체해야 한다. 예를 들어, 벌 사진과 말벌 사진을 구분하는 모델을 구분하는 모델을 훈련시키고 싶다고 하자. ImageNet에는 이 두 클래스에 해당하는 이미지가 포함돼 있어 훈련 데이터셋으로 사용될 수 있지만, 그 이미지 수가 효율적인 CNN이 과적합을 일으키지 않고 학습할 만큼 충분히 크지 않다. 그렇지만 먼저 더 광범위한 전문 지식을 개발하기 위해 전체 ImageNet 데이터셋에서 1000개의 카테고리로 분류하도록 이 네트워크를 훈련시킬 수 있다. 이 사전 훈련 과정을 거친 다음 마지막 밀집 계층을 제거하고 목표한 두 개의 클래스에 대해 예측을 출력하도록 설정된 계층으로 교체할 수 있다.

이 새로운 모델은 미리 훈련된 계층을 고정시키고 상단에 위치한 밀집 계층만 훈련시킴으로써 목표한 작업을 수행하도록 준비할 수 있다. 사실 목표 훈련 데이터셋이 너무 작기 때문에 모델에서 특징 추출기의 구성 요소를 고정시키지 않으면 모델이 과적합될 수 있다. 이 매개변수를 고정시킴으로써 네트워크가 더 풍부한 데이터셋에서 개발한 표현력을 유지할 수 있게 된다.

##### 풍부한 훈련 데이터로 유사한 작업 수행

##### 풍부한 훈련 데이터로 유사하지 않은 작업 수행

##### :pencil2: 제한적 훈련 데이터로 유사하지 않은 작업 수행

마지막으로 목표 작업이 너무 특수해서 훈련 샘플이 거의 없고 사전에 훈련된 가중치가 그다지 의미가 없는 경우, 먼저 심층 모델을 적용하고 용도에 맞게 조정하는 것이 적절한지 재검토할 필요가 있다. 그러한 모델을 작은 데이터셋에서 훈련시키면 과적합이 일어나고 깊이가 깊은 사전 훈련된 추출기는 특정 작업과는 매우 무관한 특징을 반환하게 된다. 하지만 CNN의 첫 번째 계층이 저차원 특징에 반응한다는 점을 감안하면 여전히 전이학습에서 혜택을 볼 수 있다. 사전 훈련된 모델의 최종 예측 계층만 제거하는 것이 아니라,작업에 너무 특화된 최종 합성곱 블록의 일부도 제거할 수 있다. 그런 다음 남은 계층 위에 얕은 분류기를 추가해 새로운 모델을 미세 조정할 수 있다.

### 4.3.2. 텐서플로와 케라스로 전이학습 구현

#### 모델 수술

##### 계층 제거

처음으로 할 일은 사전 훈련된 모델의 마지막 계층을 제거해 특징 추출기로 변환하는 것이다. Sequential 모델의 경우 model.layers 속성을 통해 계층 리스트에 접근할 수 있다. 이 구조에는 모델의 가장 마지막 계층을 제거하는 pop() 메서드가 있다. 따라서 네트워크를 특정 특징 추출기로 변환하기 위해 제거해야 할 마지막 계층의 개수를 안다면(예를 들어 표준 ResNet 모델의 경우 2개 계층), 다음 코드로 제거할 수 있다.

```Pythons
for i in range(num_layers_to_remove):
  model.layers.pop()
```

순수한 텐서플로에서 모델을 지원하는 연산 그래프를 편집하는 일은 단순하지도 않으며 추천할 만한 방법도 아니다. 그렇지만 사용되지 않는 그래프 연산은 런타임에 실행되지 않는다는 점에 유의해야 한다. 따라서 컴파일된 그래프에 예전 계층이 있더라도 새로운 모델에서 그 계층을 호출하지 않는 이상 계산 성능에 영향을 주지 않는다. 그러므로 계층을 제거하는 대신 이전 모델에서 유지하고자 하는 마지막 계층/연산을 정확히 특정하면 된다. 어쨋든 그 마지막 계층에 대응하는 파이썬 객체를 잃어도 그 이름을 안다면(예를 들어, 텐서보드에서 그래프를 확인해서) 이를 대표하는 텐서는 모델 계층마다 돌면서 이름을 확인함으로써 복원될 수 있다.

```Python
for layer in model.layers:
  if layer.name == name_of_last_layer_to_keep:
    bottleneck_feats = layer.output
    break
```

그렇지만 케라스는 추가 메서드를 제공해 이 절차를 단순화했다. 유지할 최종 계층의 이름을 알면(예를 들어, model.summary()로 이름을 출력한 다음) 다음 코드 2줄로 특징 추출기 모델을 구성할 수 있다.

```Python
bottleneck_feats = model.get_layer(last_layer_name).output
feature_extractor = Model(inputs=model.input, outputs=bottleneck_feats)
```

이 특징 추출 모델은 원본 모델과 이 가중치를 공유해서 사용할 준비가 됐다.

##### 계층 이식

특징 추출기 상단에 새로운 예측 계층을 추가하는 일은 텐서플로 허브를 사용했던 이전 예제에 비해 단순한데, 그에 대응하는 모델 상단에 새로운 계층을 추가하기만 하면 되기 때문이다. 예를 들어 케라스 API를 사용해 다음과 같이 추가할 수 있다.

```Python
dense1 = Dense(...)(feature_extractor.output) # ...
new_model = Model(model.input, dense1)
```

여기서 볼 수 있듯이, 텐서플로 2는 케라스를 통해 모델 길이를 줄이거나 확장하거나 결합하는 일을 단순화했다.

#### 선택적 훈련

전이학습을 사용하면 먼저 사전 훈련된 계층을 복원하고 어느 계층을 고정할지 정의해야 하기 때문에 훈련 단계가 다소 복잡해진다.

##### 사전 훈련된 매개변수 복원하기

텐서플로에는 에스티메이터를 웜스타트(warm-start)하는, 즉 사전에 훈련된 가중치를 사용해 일부 계층을 초기화하는 유틸리티 함수가 있다. 다음 코드를 사용하면 텐서플로에서 새로운 에스티메이터를 위해 동일한 이름을 공유하는 계층의 경우 사전 훈련된 에스티메이터의 저장된 매개변수를 사용할 수 있다.

```Python
def model_function():
  # ... 새로운 모델 정의, 사전 훈련된 모델을 특징 추출기로 재사용

ckpt_path = '/path/to/pretrained/estimator/model.ckpt'
ws = tf.estimator.WarmStartSetting(ckpt_path)
estimator = tf.estimator.Estimator(model_fn, warn_start_from=ws)
```

:bulb: WarmStartSetting 초기화 함수는 vars_to_warm_start를 선택적 매개변수로 취하는데, 이 변수는 체크포인트 파일에서 복원하고자 하는 특정 변수(리스트 또는 정규식으로) 이름을 제공할 때도 사용될 수 있다.

케라스를 사용하면 새로운 작업에 맞춰 변환하기 전에 사전 훈련된 모델을 복원할 수 있다.

```Python
# 사전 훈련된 모델이 `model.save()`를 사용해 저장됐다고 가정
model = tf.keras.models.load_model('/path/to/pretrained/model.h5')
# ... 그런 다음 새로운 모델을 얻기 위해 계층을 빼거나 추가함
```

일부 계층을 제거하기 위해 전체 모델을 복원하는 것이 최적은 아니지만, 이 방법이 간결하다는 장점이 있다.

##### 계층 고정하기

텐서플로에서 계층을 고정하기 위해 가장 다양하게 사용되는 기법은 최적화기에 전달되는 매개변수 리스트에서 tf.Variable 특성을 제거하는 것이다.

```Python
# 예를 들어, 이름에 "conv"가 포함된 모델 계층을 고정하고자 함

vars_to_train = model.trainable_variables
vars_to_train = [v for v in vars_to_train if "conv" in v.name]
```

케라스 계층에는 .trainable 특성이 있고 그 계층을 고정하기 위해 이 속성을 False로 설정하면 된다.

```Python
for layer in feature_extractor_model.layers:
  layer.trainable = False # 전체 추출기를 고정
```

## 4.4. 요약

- 최신 솔루션을 재사용하는 방법
- 알고리즘 자체가 이전 작업에서 획득한 지식을 통해 어떤 혜택을 얻을 수 있는지
- 전이학습을 사용하면 특정 애플리케이션을 위한 CNN 성능을 크게 향상시킬 수 있음
  - 객체 탐지를 위해 데이터셋에 주석을 다는 일은 이미지 단위 인식을 위한 주석 작업보다 더 지루해서 일반적으로 이 기법들은 더 작은 데이터셋에서 훈련됨
- 효율적인 모델을 얻기 위한 솔루션으로 전이학습을 고려하는 것이 중요함

## 4.5. 질문

## 4.6. 참고 문헌
