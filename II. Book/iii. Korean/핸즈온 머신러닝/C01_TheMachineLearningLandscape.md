# CHAPTER 1 한눈에 보는 머신러닝

광학 문자 판독기(Optical Character Recognition, OCR) 같은 특별한 몇 가지 애플리케이션이 수십 년 동안 사용되어 왔습니다. 수억 명의 생활을 편리하게 만들어 주류가 된 첫 번째 머신러닝 애플리케이션은 1990년대에 시작되었습니다. 바로 스팸 필터(Spam Filter)입니다. 스스로 생각하는 스키아넷(Skynet) 정도는 아니지만 기술적으로 머신러닝이라 할 수 있습니다(실제로 잘 학습되어 있어서 여러분은 더 이상 이메일에 스팸이라고 표시할 일이 거의 없습니다). 이후 추천과 음성 검색으로 발전했으며, 매일 사용하는 많은 제품과 기능을 소리 없이 향상시킨 수백 개의 머신러닝 애플리케이션이 나왔습니다.

## 1.1 머신러닝이란?
머신러닝은 데이터로부터 학습하도록 컴퓨터를 프로그래밍하는 과학(또는 예술)입니다.

조금 더 일반적인 정의는 다음과 같습니다.

"머신러닝은 명시적인 프로그래밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야다." - 아서 사무엘(Arthur Samuel, 1959)

조금 더 공학적인 정의는 다음과 같습니다.

"어떤 작업 T에 대한 컴퓨터 프로그램의 성능을 P로 측정했을 때 경험 E로 인해 성능이 향상됐다면, 이 컴퓨터 프로그램은 작업 T와 성능 측정 P에 대해 경험 E로 학습한 것이다." - 톰 미첼(Tom Mitchell, 1997)

## 1.2 왜 머신러닝을 사용하는가?
전통적인 접근 방법에서는 문제가 단순하지 않아 규칙이 점점 길고 복잡해지므로 유지 보수하기 매우 힘들어집니다. 반면 머신러닝 기법에 기반을 둔 스팸 필터는 일반 메일에 비해 스팸에 자주 나타나는 패턴을 감지하여 어떤 단어와 구절이 스팸 메일을 판단하는 데 좋은 기준인지 자동으로 학습합니다. 그러므로 프로그램이 훨씬 짧아지고 유지 보수하기 쉬우며 대부분 정확도가 더 높습니다.

머신러닝이 유용한 또 다른 분야는 전통적인 방식으로는 너무 복잡하거나 알려진 알고리즘이 없는 문제입니다.

우리는 머신러닝을 통해 배울 수도 있습니다. 가끔 예상치 못한 연관 관계나 새로운 추세가 발견되기도 해서 해당 문제를 더 잘 이해하도록 도와줍니다.

머신러닝 기술을 적용해서 대용량 데이터를 분석하면 겉으로는 보이지 않던 패턴을 발견할 수 있습니다. 이를 데이터 마이닝(Data Mining)이라고 합니다.

요약하면 머신러닝은 다음 분야에 뛰어납니다.
- 기존 솔루션으로는 많은 수동 조정과 규칙이 필요한 문제: 하나의 머신러닝 모델이 코드를 간단하고 더 잘 수행되도록 할 수 있습니다.
- 전통적인 방식으로는 전혀 해결 방법이 없는 복잡한 문제: 가장 뛰어난 머신러닝 기법으로 해결 바업을 찾을 수 있습니다.
- 유동적인 환경: 머신러닝 시스템은 새로운 데이터에 적응할 수 있습니다.
- 복잡한 문제와 대량의 데이터에서 통찰 얻기

## 1.3 머신러닝 시스템의 종류
- 사람의 감독 하에 훈련하는 것인지 그렇지 않은 것인지(지도, 비지도, 준지도, 강화 학습)
- 실시간으로 점진적인 학습을 하는지 아닌지(온라인 학습과 배치 학습)
- 단순하게 알고 있는 데이터 포인트와 새 데이터 포인트를 비교하는 것인지 아니면 훈련 데이터셋에서 과학자들처럼 패턴을 발견하여 예측 모델을 만드는지(사례 기반 학습과 모델 기반 학습)

### 1.3.1 지도 학습과 비지도 학습
학습하는 동안의 감독 형태나 정보량에 따라 분류할 수 있습니다.

#### 지도 학습
- k-최근접 이웃(K-Nearest Neighbors)
- 선형 회귀(Linear Regression)
- 로지스틱 회귀(Logistic Regression)
- 서포트 벡터 머신(Support Vector Machine, SVM)
- 결정 트리(Decision Tree)와 랜덤 포레스트(Random Forest)
- 신경망(Neural Networks)

#### 비지도 학습
- 군집(Clustering)
  - k-평균(k-Means)
  - 계층 군집 분석(Hierarchical Cluster Analysis, HCA)
  - 기댓값 최대화(Expectation Maximization)
- 시각화(Visualization)와 차원 축소(Dimensionality Reduction)
  - 주성분 분석(Principal Component Analysis, PCA)
  - 커널(Kernel) PCA
  - 지역적 선형 임베딩(Locally-Linear Embedding, LLE)
  - t-SNE(t-distributed Stochastic Neighbor Embedding)
- 연관 규칙 학습(Association Rule Learning)
  - 어프라이어리(Apriori)
  - 이클렛(Eclat)

#### 준지도 학습
어떤 알고리즘은 레이블이 일부만 있는 데이터도 다룰 수 있습니다. 보통은 레이블이 없는 데이터가 많고 레이블이 있는 데이터는 아주 조금입니다. 이를 준지도 학습(Semisupervised Learning)이라고 합니다.

대부분의 준비도 학습 알고리즘은 지도 학습과 비지도 학습의 조합으로 이루어져 있습니다. 예를 들어 심층 신뢰 신경망(Deep Belief Network, DBN)은 여러 겹으로 쌓은 제한된 볼츠만 머신(Restricted Boltzmann Machine, RBM)이라 불리는 비지도 학습에 기초합니다. RBM이 비지도 학습 방식으로 순차적으로 훈련된 다음 전체 시스템이 지도 학습 방식으로 세밀하게 조정됩니다.

#### 강화학습

### 1.3.2 배치 학습과 온라인 학습

#### 배치 학습
배치 학습(Batch Learning)에서는 시스템이 점진적으로 학습할 수 없습니다. 가용한 데이터를 모두 사용해 훈련시켜야 합니다. 일반적으로 이 방식은 시간과 자원을 많이 소모하므로 보통 오프라인에서 수행됩니다. 먼저 시스템을 훈련시키고 그런 다음 제품 시스템에 적용하면 더 이상의 학습 없이 실행됩니다. 즉, 학습한 것을 단지 적용만 합니다. 이를 오프라인 학습(Offline Learning)이라고 합니다.

#### 온라인 학습
온라인 학습(Online Learning)에서는 데이터를 순차적으로 한 개씩 또는 미니 배치(Mini-batch)라 부르는 작은 묶음 단위로 주입하여 시스템을 훈련시킵니다. 매 학습 단계가 빠르고 비용이 적게 들어 시스템은 데이터가 도착하는 대로 즉시 학습할 수 있습니다.

온라인 학습은 연속적으로 데이터를 받고(예를 들면 주식가격) 빠른 변화에 스스로 적응해야 하는 시스템에 적합합니다. 컴퓨팅 자원이 제한된 경우에도 좋은 선택입니다. 온라인 학습 시스템이 새로운 데이터 샘플을 학습하면 학습이 끝난 데이터는 더 이상 필요하지 않으므로 버리면 됩니다(이전 상태로 되돌릴 수 있도록 데이터를 재사용하기 위해 보관할 필요가 없다면). 이렇게 되면 많은 공간을 절약할 수 있습니다.

컴퓨터 한 대의 메인 메모리에 들어갈 수 없는 아주 큰 데이터셋을 학습하는 시스템에도 온라인 학습 알고리즘을 사용할 수 있습니다(이를 외부 메모리[out-of-core] 학습이라고 합니다). 알고리즘이 데이터 일부를 읽어 들이고 훈련 단계를 수행합니다. 전체 데이터가 모두 적용될 때까지 이 과정을 반복합니다. 

:bulb: 이 경우 전체 프로세스는 보통 오프라인으로 실행됩니다(즉, 실시간 시스템에서 수행되는 것이 아닙니다). 그래서 온라인 학습이란 이름이 혼란을 줄 수 있습니다. 점진적 학습(Incremental Learning)이라고 생각하세요.

### 1.3.3 사례 기반 학습과 모델 기반 학습

#### 사례 기반 학습
아마도 가장 간단한 형태의 학습은 단순히 기억하는 것입니다.

스팸 메일과 동일한 메일을 스팸이라고 지정하는 대신 스팸 메일과 매우 유사한 메일을 구분하도록 스팸 필터 프로그램할 수 있습니다. 이렇게 하려면 두 메일 사이의 유사도(Similarity)를 측정해야 합니다. 두 메일 사이의 매우 간단한 유사도 측정 방법은 공통으로 포함한 단어의 수를 세는 것입니다. 스팸 메일과 공통으로 가지고 있는 단어가 많으면 스팸으로 분류합니다.

이를 사례 기반 학습(Instance-based Learning)이라고 합니다. 시스템이 사례를 기억함으로써 학습합니다. 그리고 유사도 측정을 사용해 새로운 데이터에 일반화합니다.

#### 모델 기반 학습
샘플로부터 일반화시키는 다른 방법은 이 샘플들의 모델을 만들어 예측에 사용하는 것입니다. 이를 모델 기반 학습(Model-based Learning)이라고 합니다.

모델이 얼마나 좋은지 측정하는 효용 함수(Utility Function 또는 적합도 함수[Fitness Function])를 정의하거나 얼마나 나쁜지 측정하는 비용 함수(Cost Function)를 정의할 수 있습니다. 여기에서 선형 회귀 알고리즘이 등장합니다. 알고리즘에 훈련 데이터를 공급하면 데이터에 가장 잘 맞는 선형 모델의 파라미터를 찾습니다. 이를 모델을 훈련(Training) 시킨다고 말합니다.

모든 게 다 잘되면 모델은 좋은 예측을 내놓을 것입니다. 아니면 더 많은 특성(고용률, 건강, 대기오염 등)을 사용하거나, 좋은 훈련 데이터를 더 많이 모으거나, 더 강력한 모델(예를 들면 다항 회귀 모델)을 선택해야 할지 모릅니다.

지금까지의 작업을 요약해보겠습니다.
- 데이터를 분석합니다.
- 모델을 선택합니다.
- 훈련 데이터로 모델을 훈련시킵니다(즉, 학습 알고리즘이 비용 함수를 최소화하는 모델 파라미터를 찾습니다).
- 마지막으로 새로운 데이터에 모델을 적용해 예측을 하고(이를 추론, Inference이라고 합니다). 이 모델이 잘 일반화되길 기대합니다.

## 1.4 머신러닝의 주요 도전 과제

### 1.4.1 충분하지 않는 양의 훈련 데이터

:bulb: 믿을 수 없는 데이터의 효과

### 1.4.2 대표성 없는 훈련 데이터

:bulb: 유명한 샘플링 편향 사례

### 1.4.3 낮은 품질의 데이터

### 1.4.4 관련 없는 특성
성공적인 머신러닝 프로젝트의 핵심 요소는 훈련에 사용할 좋은 특성들을 찾는 것입니다. 이 과정을 특성 공학(Feature Engineering)이라 하며 다음 작업을 포함합니다.
- 특성 선택(Feature Selection): 가지고 있는 특성 중에서 훈련에 가장 유용한 특성을 선택합니다.
- 특성 추출(Feature Extraction): 특성을 결합하여 더 유용한 특성을 만듭니다(앞서 본 것처럼 차원 축소 알고리즘을 사용할 수 있습니다).
- 새로운 데이터를 수집해 새 특성을 만듭니다.

### 1.4.5 훈련 데이터 과대적합

:bulb: 과대적합은 훈련 데이터에 있는 잡음의 양에 비해 모델이 너무 복잡할 때 일어납니다. 해결 방법은 다음과 같습니다.
- 파라미터 수가 적은 모델을 선택하거나(예를 들면 고차원 다항 모델보다 선형 모델), 훈련 데이터에 있는 특성 수를 줄이거나, 모델에 제약을 가하여 단순화시킵니다.
- 훈련 데이터를 더 많이 모읍니다.
- 훈련 데이터의 잡음을 줄입니다(예를 들면 오류 데이터 수정과 이상치 제거).

### 1.4.6 훈련 데이터 과소적합

### 1.4.7 한걸음 물러서서

## 1.5 테스트와 검증

:bulb: 보통 데이터의 80%를 훈련에 사용하고 20%는 테스트용으로 떼어놓습니다.

:bulb: 공짜 점심 없음 이론

경험하기 전에 더 잘 맞을 거라고 보장할 수 있는 모델은 없습니다(이 이론의 이름이 유래된 이유입니다). 어떤 모델이 최선인지 확실히 아는 유일한 방법은 모든 모델을 평가해보는 것 뿐입니다.

## 1.6 연습문제
