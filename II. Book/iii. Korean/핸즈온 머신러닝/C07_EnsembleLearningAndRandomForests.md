# CHAPTER 7 앙상블 학습과 랜덤 포레스트
무작위로 선택된 수천 명의 사람에게 복잡한 질문을 하고 대답을 모든다고 가정합시다. 많은 경우 이렇게 모은 답이 전문가의 답보다 낫습니다. 이를 대중의 지혜<sup>wisdom of the crowd</sup>라고 합니다. 이와 비슷하게 일련의 예측기(즉, 분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있을 것입니다. 일련의 예측기를 앙상블이라고 부르기 때문에 이를 앙상블 학습<sup>Ensemble Learning</sup>이라고 하며, 앙상블 학습 알고리즘을 앙상블 방법<sup>Ensemble method</sup>이라고 합니다.

예를 들어 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 틜 분류기를 훈련시킬 수 있습니다. 예측을 하려면 모든 개별 트리의 예측을 구하면 됩니다. 그런 다음 가장 많은 선택을 받은 클래스를 예측으로 삼습니다. 결정 트리의 앙상블을 랜덤 포레스트<sup>Random Forest</sup>라고 합니다. 간단한 방법임에도 랜덤 포레스트는 오늘날 가장 강력한 머신러닝 알고리즘 중 하나입니다.

게다가 프로젝트의 마지막에 다다르면 흔히 앙상블 방법을 사용하여 이미 만든 여러 괜찮은 예측기를 연결하여 더 좋은 예측기를 만듭니다. 사실 머신러닝 경연 대회에서 우승하는 솔루션은 여러 가지 앙상블 방법을 사용한 경우가 많습니다. 이는 특히 넷플릭스 대회에서 가장 인기 있습니다.

이 장에서는 배깅, 부스팅, 스태킹 등 가장 인기 있는 앙상블 방법을 설명하겠습니다. 랜덤 포레스트도 다룰 것입니다.

## 7.1 투표 기반 분류기

## 7.2 배깅과 페이스팅
훈련 세트에서 중복을 허용하여 샘플링하는 방식을 배깅<sup>bagging</sup>(bootstrap aggregating의 줄임말)이라 하며, 중복을 허용하지 않고 샘플링하는 방식을 페이스팅<sup>pasting</sup>이라고 합니다.

다시 말해 배깅과 페이스팅에서는 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있습니다. 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있습니다.
