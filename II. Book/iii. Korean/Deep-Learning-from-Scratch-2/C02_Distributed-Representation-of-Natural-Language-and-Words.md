# Chapter 2. Distributed Representation of Natural Language and Words

컴퓨터가 우리의 말을 알아듣게(이해하게) 만드는 것

고전적인 기법(딥러닝 등장 이전의 기법)

딥러닝(신경망) 기반 기법들

파이썬으로 텍스트를 다루는 연습, 텍스트를 단어로 분할하는 처리, 단어를 단어 ID로 변환하는 처리 등을 구현

## 2.1. 자연어 처리란

평소에 쓰는 말을 자연어(natural language), 우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야), 사람의 말을 컴퓨터가 이해하도록 만들어서, 컴퓨터가 우리에게 도움이 되는 일을 수행하게 하는 것

프로그래밍 언어, 마크업 언어, 모든 코드의 의미를 고유하게 해석할 수 있도록 문법이 정의, 컴퓨터는 이 정해진 규칙에 따라서 코드를 해석

일반적인 프로그래밍 언어는 기계적, 고정, 딱딱한 언어, 자연어는 부드러운 언어, 똑같은 의미의 문장도 여러 형태로 표현할 수 있다거나, 문장의 뜻이 애매할 수 있다거나, 그 의미나 형태가 유연하게 바뀐다는 뜻, 세월이 흐르면서 새로운 말이나 새로운 의미가 생겨나거나 있던 것이 사라짐

자연어는 살아 있는 언어, 부드러움, 검색 엔진이나 기계 번역, 질의응답 시스템, IME(입력기 전환), 문장 자동요약과 감정분석 등

### 2.1.1. 단어의 의미

말은 문자로 구성되며, 말의 의미는 단어로 구성, 단어는 의미의 최소 단위, 자연어를 컴퓨터에게 이해시키는 데는 무엇보다 단어의 의미를 이해시키는 게 중요

컴퓨터에게 단어의 의미 이해시키기, 단어의 의미를 잘 파악하는 표현 방법

- 시소러스를 활용한 기법 (이전 장)
- 통계 기반 기법 (이번 장)
- 추론 기반 기법(word2vec) (다음 장)

사람 손으로 만든 시소러스(thesaurus, 유의어 사전)를 이용하는 방법, 통계 정보로부터 단어를 표현하는 통계 기반 기법, 신경망을 활용한 추론 기반 기법(구체적으로는 word2vec)

## 2.2. 시소러스

단어의 의미를 나타내는 방법으로는 먼저 사람이 직접 단어의 의미를 정의하는 방식을 생각, 그중 한 방법으로는 표준국어대사전처럼 각각의 단어에 그 의미를 설명해 넣을 수 있을 것

자연어 처리의 역사를 되돌아보면 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있어왔음, 단, 표준국어대사전 같이 사람이 이용하는 일반적인 사전이 아니라 시소러스 형태의 사전을 애용함, 시소러스란 (기본적으로는) 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류

또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등, 더 세세한 관계까지 정의해둔 경우가 있음. 각 단어의 관계를 그래프 구조로 정의

모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있음. 그러면 이 '단어 네트워크'를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수 있음. 이 정도면 컴퓨터에게 단어의 의므를 (간접적으로라도) 이해시켰다고 주장할 수 있을 것.

:bulb: 시소러스를 어떻게 사용하는가는 자언어 처리 애플리케이션에 따라 다름.

### 2.2.1. WordNet

자연어 처리 분야에서 가장 유명한 시소러스는 'WordNet'. WordNet은 프린스턴 대학교에서 1085년부터 구축하기 시작한 전통 있는 시소러스. 많은 연구와 다양한 자연어 처리 애플리케이션에서 활용.

WordNet을 사용하면 유의어를 얻거나 '단어 네트워크'를 이용할 수 있음. 또한 단어 네트워크를 사용해 단어 사이의 유사도를 구할 수도 있음. '부록 B. WordNet 맛보기'. WordNet(정확하게는 NLTK 모듈)

:bulb: 실제로 WordNet을 사용하여 단어의 유사도를 구하는 실험. 사람이 직접 정의한 '단어 네트워크'를 기초로 단어 사이의 유사도를 구하는 사례, 단어의 유사도를 (어느 정도 정확하게) 구할 수 있다면 '단어의 의미'를 이해하는 첫걸음을 내디뎠다고 말할 수 있을 것.

### 2.2.2. 시소러스의 문제점

사람이 수작업으로 레이블링하는 방식에는 크나큰 결점이 존재

- 시대 변화에 대응하기 어렵다.

때때로 새로운 단어가 생겨나고, 옛말은 언젠가 잊혀짐.

시대에 따라 언어의 의미가 변하기도 함. 이런 단어의 변화에 대응하려면 시소러스를 사람이 수작업으로 끊임없이 갱신해야 함.

- 사람을 쓰는 비용은 크다.

시소러스를 만드는 데는 엄청난 인적 비용이 발생. 현존하는 영어 단어의 수는 1,000만 개가 넘음. 이상적으로는 이 방대한 단어들 모두에 대해 단어 사이의 관계를 정의해줘야 함. WordNet에 등록된 단어는 20만 개 이상.

- 단어의 미묘한 차이를 표현할 수 없다.

시소러스에서는 뜻이 비슷한 단어들을 묶음. 그러나 실제로 비슷한 단어들이라도 미묘한 차이가 있는 법.

시소러스를 활용하는 기법(단어의 의미를 사람이 정의하는 기법)에는 문제가 있음. 이 문제를 피하기 위해, '통계 기반 기법'과 신경망을 사용한 '추론 기반 기법'을 알아볼 것. 이 두 기법에서는 대량의 텍스트 데이터로부터 '단어의 의미'를 자동으로 추출. 그 덕분에 사람은 손수 단어를 연결짓는 중노동에서 해방.

:bulb: 자연어 처리뿐 아니라, 이미지 인식에서도 특징(feature)을 사람이 수동으로 설계하는 일이 오랜 세월 계속됨. 그러다가 딥러닝이 실용화되면서 실생활 이미지로부터 원하는 결과를 곧바로 얻을 수 있게 됨. 사람이 개입할 필요가 현격히 줄어든 것. 즉, 사람의 개입을 최소로 줄이고 텍스트 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 바뀜.

## 2.3. 통계 기반 기법

말뭉치(corpus)를 이용. 말뭉치란 간단히 말하면 대량의 텍스트 데이터. 다만 맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터르 일반적으로 '말뭉치'라고 함.

결국 말뭉치란 텍스트 데이터, 그 안에 담긴 문장들은 사람이 쓴 글. 말뭉치에는 자연어에 대한 사람의 '지식'이 충분히 담겨 있다고 볼 수 있음. 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식이 포함되어 있음. 통계 기반 기법의 목표는 이처럼 사람의 지식으로 가득한 말뭉치에서 자동으로, 효율적으로 그 핵심을 추출하는 것.

:bulb: 자연어 처리에 사용되는 말뭉치에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있음. 텍스트 데이터의 단어 각각에 '품사'가 레이블링될 수 있음. 이럴 경우 말뭉치는 컴퓨터가 다루기 쉬운 형태(트리 구조 등)로 가공되어 주어지는 것이 일반적임. 이 책에서는 이러한 추가 레이블을 이용하지 않고, 단순한 텍스트 데이터(하나의 큰 텍스트 파일)로 주어졌다고 가정.

### 2.3.1. 파이썬으로 말뭉치 전처리하기

자연어 처리에는 다양한 말뭉치가 사용됨. 위키백과(Wikipedia)와 구글 뉴스(Google News) 등의 텍스트 데이터. 셰익스피어나 나쓰메 소세키 같은 작품들도 말뭉치로 이용됨. 우선 문장 하나로 이뤄진 단순한 텍스트를 사용함. 

파이썬의 대화 모드를 이용하여 매우 작은 텍스트 데이터(말뭉치)에 전처리(preprocessing). 전처리란 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일.

```Python
>>> text = 'You say goodby and I say hello.'
```

쉽게 설명하기 위해 작은 텍스트 데이터만으로 전처리를 수행함. 이 text를 단어 단위로 분할함.

```Python
>>> text = text.lower()
>>> text = text.replace('.', ' .')
>>> text
'you say goodby and i say hello .'
>>> words = text.split(' ')
>>> words
['you', 'say', 'goodby', 'and', 'i', 'say', 'hello', '.']
```

lower() 메서드를 사용해 모든 문자를 소문자로 변환. 문장 첫머리의 대문자로 시작하는 단어도 소문자 단어와 똑같이 취급하기 위한 조치. 그리고 split(' ') 메서드를 호출해 공백을 기준으로 분할. 문장 끝의 마침표(.)를 고려해 마침표 앞에 공백을 삽입한 다음 분할을 수행.

:bulb: 단어를 분할할 때 마침표 앞에 공백을 넣는 임시변통을 적용, 범용적인 방법. '정규표헌식(regular expression)'을 이용하는 방법. 정규표현식 모듈인 re를 임포트하고 re.split('(\W+)?'.text)라고 호출하면 단어 단위로 분할할 수 있음.

원래 문장을 단어 목록 형태로 이용할 수 있게 됨. 단어 단위로 분할되어 다루기가 쉬워졌지만, 단어를 텍스트 그대로 조작하기란 불편. 단어에 ID를 부여하고, ID의 리스트로 이용할 수 있도록 한 번 더 손질. 파이썬의 딕셔너리를 이용하여 단어 ID와 단어를 짝지어주는 대응표를 작성.

```Python
>>> word_to_id = {}
>>> id_to_word = {}
>>>
>>> for word in words:
...   if word not in word_to_id:
...     new_id = len(word_to_id)
...     word_to_id[word] = new_id
...     id_to_word[new_id] = word
```

단어 ID에서 단어로의 변환은 id_to_word가 담당하며(키가 단어 ID, 값이 단어), 단어에서 단어 ID로의 변환은 word_to_id가 담당. 단어 단위로 분할된 words의 각 원소를 처음부터 하나씩 살펴보면서, 단어가 word_to_id에 들어 있지 않으면 word_to_id와 id_to_word 각각에 새로운 ID와 단어를 추가. 추가 시점의 딕셔너리 길이가 새로운 단어의 ID로 설정되기 때문에 단어 ID는 0, 1, 2, .. 식으로 증가.

```Python
>>> id_to_word
{0: 'you', 1: 'say', 2: 'goodby', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
>>> word_to_id
{'you': 0, 'say': 1, 'goodby': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}
```

딕셔너리를 사용하면 단어를 가지고 단어 ID를 검색하거나, 반대로 단어 ID를 가지고 단어를 검색할 수 있음.

```Python
>>> id_to_word[1]
'say'
>>> word_to_id['hello']
5
```

'단어 목록'을 '단어 ID 목록'으로 변경. 파이썬의 내포(comprehension) 표기를 사용하여 단어 목록에서 단어 ID 목록으로 변환한 다음, 다시 넘파이 배열로 변환.

```Python
>>> import numpy as np
>>> corpus = [word_to_id[w] for w in words]
>>> corpus = np.array(corpus)
>>> corpus
array([0, 1, 2, 3, 4, 1, 5, 6])
```

:bulb: 내포란 리스트나 딕셔너리 등의 반복문 처리를 간단하게 쓰기 위한 기법. xs = [1, 2, 3, 4]라는 리스트의 각 원소를 제곱하여 새로운 리스트를 만들고 싶다면 [x**2 for x in xs]처럼 쓰면 됨.

말뭉치를 이용하기 위한 사전 준비. 이상의 처리를 한 데 모아 preprocess()라는 함수로 구현(common/util.py)

```Python
def preprocess(text):
  text = text.lower()
  text = text.replace('.', ' .')
  words = text.split(' ')
  word_to_id = {}
  id_to_word = {}
  for word in words:
    if word not in word_to_id:
      new_id = len(word_to_id)
      word_to_id[word] = new_id
      id_to_word[new_id] = word
  corpus = np.array([word_to_id[w] for w in words])
  return corpus, word_to_id, id_to_word
```

```Python
>>> text = 'You say goodbye and I say hello.'
>>> corpus, word_to_id, id_to_word = preprocess(text)
```

말뭉치를 다룰 준비를 마침. 다음 목표는 말뭉치를 사용해 '단어의 의미'를 추출하는 것. 그 한 벙법으로, 이번 절에서는 '통계 기반 기법'. 단어를 벡터로 표현.

### 2.3.2. 단어의 분산 표현

'단어의 의미'를 정확하게 파악할 수 있는 벡터 표현. 자연어 처리 분야에서는 단어의 분산 표현(distributional representation)이라 함.

:bulb: 단어의 분산 표현은 단어를 고정 길이의 밀집벡터(dense vector)로 표현. 밀집벡터라 함은 대부분의 원소가 0이 아닌 실수인 벡터.

### 2.3.3. 분포 가설

'단어의 의미는 주변 단어에 의해 형성된다'. 이를 분포 가설(distributional hypothesis)이라 하며, 단어를 벡터로 표현하는 최근 연구도 대부분 이 가설에서 기초.

단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락(context)'이 의미를 형성한다는 것. 의미가 같은 단어들은 같은 맥락에서 더 많이 등장. 이번 장에서는 '맥락'이라 하면 (주목하는 단어) 주변에 놓인 단어를 가리킴. 좌우의 각 두 단어씩이 '맥락'에 해당.

윈도우 크기가 2인 '맥락'

'맥락'이란 특정 단어를 중심에 둔 그 주변 단어를 말함. 맥락의 크기 (주변 단어를 몇 개나 포함할지)를 '윈도우 크기(window size)'라 함. 윈도우 크기가 1이면 좌우 한 단어씩이, 윈도우 크기가 2이면 좌우 두 단어씩이 맥락에 포함됨.

:bulb: 좌우를 똑같은 수의 단어를 맥락으로 사용함. 하지만 상황에 따라서는 왼쪽 단어만 또는 오른쪽 단어만을 사용하기도 하며, 문장의 시작과 끝을 고려할 수도 있음. 이 책에서는 이해하기 쉽게 설명하고자 문장 구분은 고려하지 않고 좌우 동수인 맥락막을 취급함.

### 2.3.4. 동시발생 행렬

분포 가설에 기초해 단어를 벡터로 나타내는 방법. 주변 단어를 '세어보는' 방법. 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법. 이를 '통계 기반(statistical based)' 기법.

```Python
import sys
sys.path.append('..')
import numpy as np
from common.util import preprocess
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)
print(id_to_word)
```

모든 단어에 대해 동시발생하는 단어를 표에 정리. 이 표의 각 행은 해당 단어를 표현한 벡터가 됨. 이 표가 행렬의 형태를 띤다는 뜻에서 동시발생 행렬(co-occurrence matrix)이라 함.

```Python
C = np.array([
  [0, 1, 0, 0, 0, 0, 0],
  [1, 0, 1, 0, 1, 1, 0],
  [0, 1, 0, 1, 0, 0, 0],
  [0, 0, 1, 0, 1, 0, 0],
  [0, 1, 0, 1, 0, 0, 0],
  [0, 1, 0, 0, 0, 0, 1],
  [0, 0, 0, 0, 0, 1, 0],
], dtype=np.int32)
```

동시발생 행렬을 활용하면 단어를 벡터로 나타낼 수 있음. 자동화할 수 있음. 말뭉치로부터 동시발생 행렬을 만들어주는 함수를 구현. create_co_matrix(corpus, vocab_size, window_size=1). 단어 ID의 리스트, 어휘 수, 윈도우 크기(common/util.py)

```Python
def create_co_matrix(corpus, vocab_size, window_size=1):
  corpus_size = len(corpus)
  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
  
  for idx, word_id in enumerate(corpus):
    for i in range(1, window_size + 1):
      left_idx = idx - 1
      right_idx = idx + 1
      
      if left_idx >= 0:
        left_word_id = corpus[left_idx]
        co_matrix[word_id, left_word_id] += 1
      
      if right_idx < corpus_size:
        right_word_id = corpus[right_idx]
        co_matrix[word_id, right_word_id] += 1
  
  return co_matrix
```

먼저 co_matrix를 0으로 채워진 2차원 배열로 초기화. 말뭉치의 모든 단어 각각에 대하여 윈도우에 포함된 주변 단어를 세어나감. 이때 말뭉치의 왼쪽 끝과 오른쪽 끝 경계를 벗어나지 않는지도 확인.

이 함수는 말뭉치가 아무리 커지더라도 자동으로 동시발생 행렬을 만들어줌.

### 2.3.5. 벡터 간 유사도

벡터 사이의 유사도를 측정하는 방법.

벡터 사이의 유사도를 측정하는 방법은 다양. 대표적으로는 벡터의 내적이나 유클리드 거리 등. 코사인 유사도(cosine similarity)를 자주 이용함. 두 벡터 x=(x1, x2, x3, ..., xn)과 y=(y1, y2, y3, ..., yn)이 있다면, 코사인 유사도는 다음 식으로 정의됨.

similarity(x, y) = x1 * y1 + ... + xn * yn / ((x1^2 + ... + xn^2)^(1/2) * (y1^2 + ... + yn^2)^(1/2)) ... [식 2.1]

분자에는 벡터의 내적이, 분모에는 각 벡터의 노름(norm)이 등장. 노름은 벡터의 크기를 나타낸 것, 'L2 노름'을 계산함(L2 노름은 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 구해 계산함). [식 2.1]의 핵심은 벡터를 정규화하고 내적을 구하는 것임.

코사인 유사도를 파이썬 함수로 구현(common/util.py)

```Python
def cos_similarity(x, y):
  nx = x / np.sqrt(np.sum(x**2)) # x의 정규화
  ny = y / np.sqrt(np.sum(y**2)) # y의 정규화
  return np.dot(nx, dy)
```

인수 x와 인수 y는 넘파이 배열이라고 가정. 이 함수는 먼저 벡터 x와 y를 정규화한 후 두 벡터의 내적을 구함. 문제. 인수로 제로 벡터(원소가 모두 0인 벡터)가 들어오면 '0으로 나누기(divide by zero)' 오류가 발생.

해결하는 전통적인 방법은 나눌 때 분모에 작은 값을 더해주는 것. 작은 값을 뜻하는 eps를 인수로 받도록 하고, 이 인수의 값을 지정하지 않으면 기본값으로 1e-8(=0.00000001)이 설정되도록 수정(eps는 엡실론(epsilon)의 약어). 개선된 코드(common/util.py)

```Python
def cos_similarity(x, y, eps=1e-8):
  nx = x / (np.sqrt(np.sum(x**2)) + eps) # x의 정규화
  ny = y / (np.sqrt(np.sum(y**2)) + eps) # y의 정규화
  return np.dot(nx, dy)
```

:bulb: 1e-8이면 일반적으로 부동소수점 계산 시 '반올림'되어 다른 값에 '흡수'됨. 이 값이 벡터의 노름에 '흡수'되기 때문에 대부분의 경부 eps를 더한다고 해서 최종 계산 결과에는 영향을 주지 않음. 벡터의 노름이 0일 때는 이 작은 값이 그대로 유지되어 '0으로 나누기' 오류가 나는 사태를 막아줌.

이 함수를 사용하면 단어 벡터의 유사도를 구할 수 있음(ch02/similarity.py)

```Python
import sys
sys.path.append('..')
from common.util import preprocess, create_co_matrix, cos_similarity

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id['you']] # "you"의 단어 벡터
c1 = C[word_to_id['i']] # "i"의 단어 벡터
print(cos_similarity(c0, c1))
```

코사인 유사도 값은 -1에서 1 사이이므로, 이 값은 비교적 높다(유사성이 크다)고 말할 수 있음.

### 2.3.6. 유사 단어의 랭킹 표시

어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수. most_similar(query, word_to_id, id_to_word, word_matrix, top=5) (common/util.py)

query: 검색어(단어)

word_to_id: 단어에서 단어 ID로의 딕셔너리

id_to_word: 단어 ID에서 단어로의 딕셔너리

word_matrix: 단어 벡터들을 한데 모은 행렬, 각 행에는 대응하는 단어의 벡터가 저장되어 있다고 가정

top: 상위 몇 개까지 출력할지 설정

```Python
def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
  # 1. 검색어를 꺼냄.
  if query not in word_to_id:
    print('%s(을)를 찾을 수 없습니다.' %query)
    return
  
  print('\n[query] ' + query)
  query_id = word_to_id[query]
  query_vec = word_matrix[query_id]
  
  # 2. 코사인 유사도 계산
  vocab_size = len(id_to_word)
  similarity = np.zeros(vocab_size)
  for i in range(vocab_size):
    similarity[i] = cos_similarity(word_matrix[i], query_vec)
  
  # 3. 코사인 유사도를 기준으로 내림차순으로 출력
  count = 0
  for i in (-1 * similarity).argsort():
    if id_to_word[i] == query:
      continue
    print(' %s: %s' % (id_to_word[i], similarity[i]))
    
    count += 1
    if count >= top:
      return
```

3에서는 similarity 배열에 담긴 원소의 인덱스를 내림차순으로 정렬한 후 상위 원소들을 출력함. 이때 배열 인덱스의 정렬을 바꾸는데 사용한 argsort() 메서드는 넘파이 배열의 원소를 오름차순으로 정렬함(단, 반환값은 배열의 인덱스).

```Python
>>> x = np.array([100, -20, 2])
>>> x.argsort()
array([1, 2, 0])
```

마이너스를 곱한 후 argsort() 메서드를 호출.

```Python
>>> (-x).argsort()
array([0, 2, 1])
```

이처럼 argsort()를 사용하면 단어의 유사도가 높은 순서로 출력할 수 있음.

동시발생 행렬을 이용하면 단어를 벡터로 표현할 수 있음. 통계 기반 기법의 '기본'. 

## 2.4. 통계 기반 기법 개선하기

더 실용적인 말뭉치를 사용하여 '진짜' 단어의 분산 표현.

### 2.4.1. 상호정보량

앞 절에서 본 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타냄. 그러나 이 '발생' 횟수라는 것은 사실 그리 좋은 특징이 아님. 

이 문제를 해결하기 위해 점별 상호정보량(pointwise mutual information, PMI)이라는 척도를 사용함. PMI는 확률 변수ㅏ x와 y에 대해 다음 식으로 정의됨. 

PMI(x, y) = log2 ( P(x, y) / (P(x) * P (y)) ) ... [식 2.2]

[식 2.2]에서 P(x)는 x가 일어난 확률, P9y)는 y가 일어날 확률, P(x, y)는 x와 y가 동시에 일어날 확률을 뜻함. 이 PMI 값이 높을수록 관련성이 높다는 의미임.

이 식을 앞의 자연어 예에 적용하면 P(x)는 단어 x가 말뭉치에 등장할 확률을 가리킴. 예를 들어 10,000개의 단어로 이워진 말뭉치에서 "the"가 100번 등장한다면 P("the") = 100/10000 = 0.01이 됨. 또한 P(x, y)는 단어 x와 y가 동시발생할 확률이므로, 마찬가지로 "the"와 "car"가 10번 동시발생했다면 P("the", "car") = 10/10000 = 0.001이 됨.

동시발생 행렬(각 원소는 동시발생한 단어의 횟수)을 사용하여 [식 2.2]를 다시 써보면, C는 동시발생 행렬, C(x, y)는 단어 x와 y가 동시발생하는 횟수, C(x)와 C(y)는 각각 단어 x와 y의 등장 횟수임. 이때 말뭉치에 포함된 단어 수를 N이라 하면, [식 2.2]는 다음과 같이 변함.

PMI(x, y) = log2( P(x, y) / (P(X) * P(y)) ) = log2( (C(x, y)/N / ((C(x)/N * (C(y)/N))) ) = log2( (C(x, y) * N) / (C(x) * C(y)) ) ... [식 2.3]

[식 2.3]에 따라 동시발생 행렬로부터 PMI를 구할 수 있음. 말뭉치 단어의 수 (N), "the", "car", "drive"가 각 10000, 1000, 20, 10번 등장했을 때, "the"와 "car"의 동시발생 수는 10회, "car"와 "drive"의 동시발생 수는 5회라 가정하면, 동시발생 횟수 관점에서는 "car"는 "drive"보다 "the"와 관련이 깊다고 나옴. PMI 계산 결과는 다음과 같음.

PMI("the", "car") = log2( (10 * 10000) / (1000 * 20) ) = 2.32 ... [식 2.4]

PMI("car", "drive") = log2( (5 * 10000) / (20 * 10) ) = 7.97 ... [식 2.5]

PMI를 이용하면 "car"는 "the"보다 "drive"와의 관련성이 강해짐. 단어가 단독으로 출현하는 횟수가 고려되었기 때문. 이 예에서는 "the"가 자주 출현했으므로 PMI 점수가 낮아진 것임.

PMI에도 한 가지 문제가 있음. 두 단어의 동시발생 횟수가 0이면 log2(0) = -inf가 된다는 점임. 이 문제를 피하기 위해 실제로 구현할 때는 양의 상호정보량(positive PMI, PPMI)을 사용함.

PPMI(x, y) = max(0, PMI(x,y)) ... [식 2.6]

이 식에 따라 PMI가 음수일 때는 0으로 취급함. 이제 단어 사이의 관련성을 0 이상의 실수로 나타낼 수 있음. ppmi(C, verbose=False, eps=1e-8) (common/util.py)

```Python
def ppmi(C, verbose=False, eps=1e-8):
```

### 2.4.2. 차원 감소

### 2.4.3. SVD에 의한 차원 감소

### 2.4.4. PTB 데이터셋

### 2.4.5. PTB 데이터셋 평가

## 2.5. 정리
