# Chapter 1. 신경망 복습

## 1.1. 수학과 파이썬 복습

### 1.1.1. 벡터와 행렬

### 1.1.2. 행렬의 원소별 연산

### 1.1.3. 브로드캐스트

### 1.1.4. 벡터와 내적과 행렬의 곱

:bulb: 벡터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'를 나타냅니다. 벡터의 길이가 1인 경우로 한정하면, 완전히 같은 방향을 두 벡터의 내적은 1이 됩니다. 반대로, 반대 방향을 향하는 두 벡터의 내적은 -1입니다.

```Python
# 벡터의 내적
>>> np.dot(a, b)

# 행렬의 곱
>>> np.matmul(A, B)
```

### 1.1.5. 행렬 형상 확인

행렬 A와 B가 대응하는 차원의 원소 수가 같아야 합니다. 그리고 결과로 만들어진 행렬 C의 형상은 A의 행 수와 B의 열 수가 됩니다. 이것이 행렬의 '형상 확인'입니다.

## 1.2. 신경망의 추론

### 1.2.1. 신경망 추론 전체 그림

신경망은 간단히 말하면 단순한 '함수'라 할 수 있습니다. 함수란 무엇인가를 입력하면 무엇인가를 출력하는 변환기죠. 다시 말해 신경망도 함수처럼 입력을 출력으로 변환합니다.

### 1.2.2. 계층으로 클래스화 및 순전파 구현

## 1.3. 신경망의 학습

추론이란 앞 절에서 본 것 같은 다중 클래스 분류 등의 문제에 답을 구하는 작업입니다. 한편, 신경망의 학습은 최적의 매개변수 값을 찾는 작업입니다.

### 1.3.1. 손실 함수

신경망 학습에는 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'가 필요합니다. 일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 손실(loss)을 사용합니다. 손실은 학습 데이터 (학습 시 주어진 정답 데이터)와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값 (스칼라)입니다.

신경망의 손실은 손실 함수(loss function)를 사용해 구합니다. 다중 클래스 분류(multi-class classification) 신경망에서는 손실 함수로 흔히 교차 엔트로피 오차(Cross Entropy Error)를 이용합니다. 교차 엔트로피 오차는 신경망이 출력하는 각 클래스의 '확률'과 '정답 레이블'을 이용해 구할 수 있습니다.

### 1.3.2. 미분과 기울기

:bulb: 엄밀하게 말하면, 이 책에서 사용하는 '기울기'는 수학에서 말하는 기울기와는 다릅니다. 수학에서의 기울기는 벡터에 대한 미분으로 한정됩니다. 한편, 딥러닝에서는 행렬이나 텐서에 대해서도 미분을 정의하고, 그것을 기울기라 부르는 것이 일반적입니다.

### 1.3.3. 연쇄 법칙

연쇄 법칙(chain rule)이란 합성함수에 대한 미분의 법칙이다.

:bulb: 신경망은 여러 '함수'가 연결된 것이라고 생각할 수 있습니다. 오차역전파법은 그 여러 함수(신경망)에 대해 연쇄 법칙을 효율적으로 적용하여 기울기를 구해냅니다.

### 1.3.4. 기울기 그래프

계산 그래프는 계산 과정을 시각적으로 보여줍니다. 

### 1.3.5. 기울기 도출과 역전파 구현

### 1.3.6. 가중치 갱신

## 1.4. 신경망으로 문제를 풀다

### 1.4.1. 스파이럴 데이터셋

### 1.4.2. 신경망 구현

### 1.4.3. 학습용 코드

### 1.4.4. Trainer 클래스

## 1.5. 계산 고속화

### 1.5.1. 비트 정밀도

### 1.5.2. GPU(쿠파이)

## 1.6. 정리
