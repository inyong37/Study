# Chapter 03 딥러닝의 표현 

## 01 풀어진 표현(Disentangled Representation)
### 풀어진 표현의 개념과 이해

### 구글의 Shape3D

### 풀어진 표현 학습의 방법

## 02 확률 분포와 함수
### 확률 분포(Probabolity Distribution)

### 확률 질량 함수(Probabilty Mass Function)

### 확률 밀도 함수(Probabilty Density Function)

## 03 확률의 가능도와 딥러닝의 확률 분포
### 확률과 가능도

### 최대 가능도 예측

### 딥러닝에서의 확률 분포 예측

## 04 베이즈 정리와 추론
### 베이즈 정리(Bayesian Theorem)

### 베이즈 추론(Bayesian Inference)

## 05 변분 오토인코더(VAE)
### 변분 오토인코더의 개념과 목표

### 변분 오토인코더의 생성기(Generator)

### 사후 확률 추정을 위한 변분 추론(Variational Inference)

### 오토인코더와 변분 오토인코더의 차이

## 06 풀어진 표현 학습
### 풀어진 표현 학습(Disentangled Representation Learning)의 개념

### 풀어진 표현과 잠재 변수

### 풀어진 표현의 성능 측정과 한계

## 07 풀어진 표현 학습의 방법론
### 베타 변분 오토인코더(b-VAE)

### InfoGAN

## 08 풀어진 표현과 비지도 학습
### 풀어진 표현 학습의 경험적 결과

### 풀어진 표현 학습의 향후 연구 주제

## 09 풀어진 표현 학습의 활용
### 호기심 기반 탐색(Curiosity Driven Exploration)

### 추상화 추론(Abstract Reasoning)

### 시각적 개념 학습(Visual Concept Learning)

## :bulb: 비지도 학습의 맹점
### 데이터 = 재화

### 비지도 학습의 매력

### 비지도 학습의 한계
