# Chapter 04 딥마인드의 알파스타
## 01 알파스타(AlphaStar)의 시작
### 스타크래프트의 인공지능

### 스타크래프트2 인공지능 개발 환경과 연구 결과

### 알파스타의 데뷔

## 02 알파스타와 알파고의 차이점
### 바둑과 스타크래프트2의 게임 장르

### 게임 장르의 차이

### 알파스타의 도전 과제

## 03 알파스타의 인공지능 알고리즘과 학습 방법
### 알파스타의 인공지능 알고리즘

### 알파스타의 지도 학습

### 알파스타의 강화 학습

### 알파스타의 리그 학습

## 04 알파스타의 입력과 출력
### 종단간(End to End) 학습

### 알파스타의 입력

### 알파스타의 출력

## 05 알파스타의 지도 학습과 에이전트
### 알파스타의 지도 학습 목표(가능한 전략)

### 알파스타의 지도 학습 구조와 에이전트

## 06 알파스타의 강화 학습
### 알파스타의 강화 학습 목표(지도 학습 에이전트의 승률 상승)

### 알파스타의 강화 학습 구조

## 07 알파스타의 리그 학습
### 알파스타의 리그 학습(게임 이론의 도전 과제)

### 알파스타의 리그 학습 에이전트

## 08 알파스타의 지도 학습 알고리즘(트랜스포머)
### 트랜스포머의 부상

### 트랜스포머의 차별성

## 09 알파스타의 지도 학습 알고리즘(포인터 네트워크)
### 포인터 네트워크의 개념과 구조

### 포인터 네트워크의 활용

### 볼록 껍질(Convex Hull) 문제

## 10 알파스타의 강화 확습 알고리즘(정책과 가치)
### 정책과 가치, 그리고 행동 가치

### 정책과 가치의 학습

## 11 알파스타의 강화 학습 알고리즘(정책 변화도)
### 정책 변화도(Policy Gradient)

### REINFORCE 알고리즘

## 12 알파스타의 강화 학습 알고리즘(액터-크리틱)
### 액터-크리틱(Actor Critic)의 개념

### 강화된 액터-크리틱(Advantage Actor Critic, A2C)

### Asynchronous Advantage Actor Critic(A3C)

## 13 알파스타의 강화 학습 알고리즘(오프 폴리시 액터-크리틱)
### 온 폴리시(On Policy)와 오프 폴리시(Off Policy)

### 오프 폴리시(Off Policy)의 장점

## 14 알파스타의 강화 학습 알고리즘(경험 리플레이)
### 경험 리플레이(Experience Replay)의 개념

### 경험 리플레이와 오프 폴리시

## 15 알파스타의 강화 학습 알고리즘(자가 모방 학습)
### 모방 학습(Imitation Learning)의 개념

### 자가 모방 학습(Self Imitation Learning)의 목표

## :bulb: 게임 인공지능
### 게임 속의 인공지능

### 게임 인공지능 연구의 장점

### 다양한 게임 인공지능의 연구 결과
