# Chapter 02 딥러닝의 한계
## 01 딥러닝과 데이터 :star:
### 딥러닝과 데이터의 관계

### 데이터의 중요성

## 02 데이터의 가치
### 21세기의 데이터

### 재화로써의 데이터

## 03 데이터와 개인정보
### 데이터와 개인정보의 활용

### 데이터를 둘러싼 주요국의 입장(개방 vs 보호)

### 한국의 입장

## 04 데이터의 수집과 알고리즘의 편향 :star:
### 데이터의 수집과 편향

### 인공지능 알고리즘의 편향

### 데이터와 알고리즘의 편향을 방지하기 위한 방안

## 05 지속적인 학습에 대한 어려움 :star:
### 딥러닝에 대한 가장 큰 오해

### 지속적인 학습의 어려움

### 인공신경망의 학습

## 06 지식 전이와 전이 학습 :star:
### 지식 전이의 개념
지식 전이(Knowledge Transfer)는 분야 간 지식을 서로 공유하거나 재사용할 수 있는지에 대한 개념이다. 이것은 전이 학습(Transfer Learning)이라는 용어로 활용되는데 전이 학습의 저변에는 학습 방법을 어떻게 일반화할 수 있는지에 대한 문제가 존재한다. 일반화가 가능하다면 동일한 메커니즘으로 이미지에서 객체나 음성을 인식할 수 있다. 그러나 현대의 딥러닝은 학습 방법이 단순하기 때문에 사람의 학습 방법과는 거리가 있다. 인공신경망을 성공적으로 학습시키기 위해서는 대량의 데이터가 필수적이다. 딥러닝을 큰 의미에서 보면 통계적인 학습이므로 많은 데이터를 보여줄수록 높은 성능을 기대할 수 있다. 반면 사람은 개념적인 학습을 하기 때문에 물체에 개념을 부여하고, 개념 간 관계를 추론하고 예측한다. 이러한 개념적인 학습은 초기 인공지능, 즉 기호 기반의 인공지능에서 주로 시도되었다. 기호 기반의 인공지능은 동일한 메커니즘으로 다양한 일을 수행할 수 있으나 인식(Perception)의 측면에서는 매우 낮은 성능을 보였다.

### 전이 학습의 활용
전이 학습은 지속적인 학습과 마찬가지로 범용 인공지능을 달성하기 위한 재료이다. 그러나 딥러닝 학계에서는 재사용의 관점에서 전이 학습이라는 단어를 활용하기도 한다. 이미 학습된 신경망 모델(Pre-trained Model)을 활용한다는 관점인데, 예를 들어 이미지 인식 경진대회에서 우승한 AlexNet의 신경망을 재사용한다고 가정할 경우 AlexNet의 가중치는 이미 온라인에 공개되어 있기 때문에 해당 가중치를 조정하는 경우이고, 미세 조정(Fine-Tuning)은 가장 마지막 분류층(Classifier)의 가중치만 재학습하는 것을 의미한다.

### 전이 학습의 사례
전이 학습에 가장 가까운 사례는 딥마인드가 개발한 알파 제로이다. 알파 제로는 알파고의 최종 업그레이드 버전인 알파고 제러에서 알고리즘을 일반화한 것이다. 알파고 제로는 인간의 기보를 전혀 학습하지 않고, 자체 대국만으로 학습하여 바둑을 정복한 인공지능이다. 알파 제로는 이러한 자체 대결을 통해 학습하는 방식을 일반화한 것이다. 비록 알파 제로는 보드 게임이라는 틀을 벗어날 수 없지만 체스, 일본식 장기 등 다양한 게임에서 그 성능을 증명하였다. 하지만 각각의 게임 규칙이 모두 다르기 때문에 게임별로 다시 학습을 해야 한다는 단점이 있다. 알파 제로를 궁극적인 의미에서 전이 학습이라고 보이는 어렵지만 혁신적인 연구 결과임에는 틀림이 없다.

### 넓은 의미에서의 전이 학습
전이 학습은 본질적인 의미에서 매우 도전적인 과제이다. 특정 데이터로 학습된 모델을 바탕으로 표현하고, 이로부터 다른 분야의 데이터를 예측하는 모델을 생성한다. 
학습 데이터 분야 -> 모델 A -> 지식 -> 모델 B -> 다른 데이터 분야

### 딥러닝에서의 전이 학습과 미세 조정
- 전이 학습(Transfer Learning) 
  - 이미 학습된 모델을 재사용한다는 의미로 활용
  - 이미지 분류라는 문제를 해결하기 위해 기존의 이미지 분류를 성공적으로 수행한 신경망을 활용
  - 이미 학습된 신경망은 마지막 층의 분류 층을 해결하기 위해 문제의 분류 개수만큼 조정하고, 이미 학습된 신경망의 가중치를 초기 조건으로 학습을 수행
  - 동일한 임무를 수행하는 신경망의 경우 데이터의 양이 많지 않더라도 학습할 수 있는 장점이 있음
- 미세 조정(Fine-Tuning)
  - 이미 학습된 신경망을 활용한다는 점에서 전이 학습과 매우 유사하지만 가중치를 갱신하는 규칙이 다름
  - 이미 학습된 신경망의 마지막 분류 층을 해당 문제의 분류로 변경하고, 이후 학습은 마지막 FC 층과 분류 층을 이어주는 가중치만을 수행
  - 해결하고자 하는 문제와 이미 학습된 신경망의 문제가 상당 부분 유사할 경우 활용되는 방법으로 학습에 소요되는 시간을 큰 폭으로 단축시킬 수 있는 장점이 있음

## 07 딥러닝과 컴퓨팅 파워
### 컴퓨팅 파워의 한계

### 그래픽 연산 처리 장치의 한계

### 딥러닝과 그래픽 연산 처리 장치와의 관계

## 08 인공신경망의 학습과 최적의 모델 :star:
### 인공신경망의 경험적 접근

### 인공신경망의 구조와 학습

### 최적의 인공신경망 모델

## 09 인공신경망의 가중치와 설명 가능성 :star:
### 인공신경망 가중치의 의미

### 인공신경망의 설명 가능성과 중요성

## 10 좁은 인공지능과 범용 인공지능 :star:
### 좁은 인공지능(Narrow Artificial Intelligence)

### 범용 인공지능(Artificial General Intelligence)

### 범용 인공지능의 접근 방법

## 범용 인공지능의 시대는 언제 올 것인가? :bulb:
### 딥러닝의 가장 큰 문제
데이터 의존도

### 범용 인공지능의 시대는 언제 올까?

### 범용 인공지능의 시대가 온다면?
Super Intelligence
