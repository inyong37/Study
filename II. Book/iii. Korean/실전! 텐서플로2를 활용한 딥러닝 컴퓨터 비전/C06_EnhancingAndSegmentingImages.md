# 06 이미지 보강 및 분할

- 인코더-디코더 개념과 픽셀-수준의 예측을 위해 훈련시키는 방법
- 고차원 데이터를 출력하기 위해 인코더-디코더에서 어떤 신규 계층(언풀링, Unpooling, 전치합성곱, Transposed Convolutions, 아트루스 합성곱, Atrous Convolutions)을 사용하는가
- FCN과 U-Net 아키텍처는 어떻게 의미론적 분할을 해결하는가
- 지금까지 알아본 모델이 인스턴스 분할을 다루기 위해 어떻게 확장될 수 있는가

## 기술 요구사항

## 인코더-디코더로 이미지 변환

### 인코더-디코더 소개

#### 인코딩과 디코딩
이 프레임워크는 인코더는 입력 샘플을 잠재 공간, 즉 인코더에 의해 정의된 숨겨둔 구조화된 값 집합에 매핑하는 함수다. 디코더는 이 잠재 공간의 요소를 사전 정의된 타깃 도메인으로 매핑하는 여함수다. 예를 들어 인코더는 미디어 파일을 파싱하기 위해 생성되고(콘텐츠는 잠재 공간의 요소로 표시됨), 가령 미디어 콘텐츠를 다른 파일 포맷으로 출력하도록 정의된 디코더와 쌍을 이룰 수 있다. 잘 알려진 예로는 오늘날 보편적으로 사용하는 이미지와 오디오 압축 포맷을 들 수 있다. JPEG 도구는 미디어 파일을 보다 가벼운 이진 파일로 압축하여 인코딩하고, 표시할 때 픽셀 값을 복원하기 위해 디코딩한다.

머신러닝에서 인코더-디코더 네트워크는 오랫동안 사용돼 왔다(예: 텍스트 번역). 인코더 네트워크는 출발어(source language)의 문장을 입력으로 취하고(예: 프랑스어 문장), 문장의 의미가 특징 벡터로 인코딩된 잠재 공간으로 이 문장을 투사하는 것을 학습한다. 디코더 네트워크는 인코딩된 벡터를 도착어(target language, 예: 영어 번역)의 문장으로 변환하기 위해 인코더와 함께 훈련된다.

:bulb: 인코더-디코더 모델에서 잠재 공간의 벡터를 일반적으로 코드(code)라고 한다.

#### 오토인코딩
오토인코더(Auto-encoder, AE)는 특수한 유형의 인코더-디코더다. 오토인코더의 입력과 타깃 도메인이 동일해서 이 모델의 병목 계층(저차원의 잠재 공간)에도 불구하고 적절하게 인코딩한 다음 품질에 영향을 주지 않고 이미지를 디코딩하는 것을 목표로 한다. 입력은 압축된 표현(특징 벡터)으로 줄어든다. 원본 입력이 나중에 요청되면 디코더에 의해 압축된 표현으로부터 재구성될 수 있다. 

:bulb: 오토인코더와 관련해 머신러닝 전문가들의 의견이 나뉜다. 일부는 이 모델이 비지도 학습이라고 주장하는데, 훈련에 추가 레이블이 필요하지 않기 때문이다. 다른 전문가들은 순수 비지도 기법(전통적으로 레이블이 없는 데이터셋에서 패턴을 발견하기 위해 복잡한 손실 함수를 사용하는)과는 다르며 AE에는 명확하게 정의된 타깃(즉, 입력 이미지)이 있다고 주장한다. 따라서 이 모델을 자기 지도 학습(Self-Supervised Learning)이라고도 한다(즉 이 모델의 타깃이 입력에서 직접 파생될 수 있다).

:bulb: 병목 조건을 제외하면 이 항등 매핑은 ResNet 같은 숏컷 경로가 있는 네트워크에서 간단하다. 인코더에서 디코더로 전체 입력 정보를 전달하기만 하면 된다. 저차원 잠재 공간(병목)을 사용하면 이 너트워크는 적절히 압축된 표현을 학습해야 한다.

#### 목적

### 기본 예제 - 이미지 노이즈 제거

#### 간단한 완전 연결 오토인코더

#### 이미지 노이즈 제거에 적용

### 합성곱 인코더-디코더

#### 언풀링, 전치, 팽창
전치 합성곱(Transposed Convolution, 디컨볼루션, Deconvolution), 팽창된 합성곱(Dilated Convolution), 언풀링(Unpooling)

##### 전치 합성곱(디컨볼루션)
형상이 (H, W, D)인 텐서 입력에 대해 출력 형상(H_o', W_o', N)을 평가하기 위해 다음 공식을 이용했다(커널 크기 k, 입력 깊이 D, 커널 개수 N, 패딩 p, 보폭 s).

H_o = (H - k + 2p) / s + 1

W_o = (W - k + 2p) / s + 1

이제 합성곱의 공간 변환을 역으로 수행하는 계층을 개발해야 한다고 가정하자. 다른 말로, 형상이 (H_o', W_o', N)인 특징 맵과 동일한 초매개변수 k, D, N, p, s가 주어졌을 때 형상이 (H, W, D)인 텐서를 복원하기 위해 합성곱과 유사한 연산이 필요하다. 이전 공식의 H와 W를 따로 분리해 보면 다음 속성을 유지하는 연산이 필요하다.

H = (H_o - 1) * s + k - 2p

W = (W_o - 1) * s + k - 2p

이것이 전치 합성곱이 정의된 방식이다. 이 새로운 유형의 계층은 ILSVRC 2013에서 우승한 기법인 ZFNet(Visualizing and Understanding Convolutional Networks)을 개발한 연구원인 매튜 제일러(Matthew Zeiler)와 롭 퍼거스(Rob Fergus)에 의해 제안됐다.

이 계층은 k * k * D * N 커널 스택을 사용해 H_o * W_o * N 텐서에 합성곱을 적용해 H * W * D 맵으로 변환한다. 이를 달성하기 위해 먼저 입력 텐서는 팽창(Dilation) 과정을 거쳐야 한다. 비율 d에 의해 정의된 팽창 연산은 입력 텐서의 행과 열의 쌍에서(개별적으로) d-1개의 0으로 채워진 행과 열을 추가하는 것으로 구성된다. 전치 합성곱에서 팽창 비율은 s(전치 합성곱이 반전시킨 표준 합성곱에서 사용된 보폭)로 설정된다. 이 재표본 추출 후에는 텐서는 p' = k - p - 1에 의해 패딩된다. 팽창과 패딩 매개변수는 모두 원본 형상 (H, W, D)를 복원하기 위해 그렇게 정의된다. 그런 다음 보폭 s'=1을 사용해 실제 텐서를 해당 계층의 필터와 합성곱을 수행하고 그 결과 H * W * D 맵을 얻게 된다.

:bulb: 컨볼루션 레이어에서 패치와 커널 사이에 실제로 일어나는 수학 연산은 교차 상관관계다.

:bulb: 디컨볼루션 레이어에서 패치와 커널 사이에 적용되는 연산은 수학적 합성곱이다.

이 프로세스가 다소 추상적으로 보일 수 있지만, 전치 합성곱 계층은 일반적으로 표준 합성곱 계층을 반전시켜 특징 맵의 콘텐츠와 훈련 가능한 필터 사이의 합성곱을 통해 특징 맵의 공간 차원을 증가시키기 위해 사용된다는 점만 기억해도 충분하다. 따라서 이 계층은 디코더 아키텍처에 상당히 부합한다. 전치 합성곱 계층은 tf.layers.conv2d_transpose()와 tf.keras.layers.Conv2DTranspose()를 사용해 인스턴스화될 수 있으며, 이 둘은 표준 conv2d 계층으로 동일한 시그니처를 갖는다.

:bulb: 표준 합성곱과 전치 합성곱 사이에는 실제로는 아무 영향이 없지만 알아두면 좋을 또 다른 미묘한 차이가 있다. CNN에서 합성곱 계층은 실제로는 교차 상관관계를 수행한다. 전치 합성곱 계층은 커널 인덱스를 거꾸로 하여 수학적 합성곱 연산을 사용한다. 전치 합성곱은 잘못된 명칭이기는 하지만 일반적으로 디컨볼루션이라고도 한다. '디컨볼루션'이라는 이름의 수학 연산이 있지만, 이는 전치 합성곱과는 다르다. 디컨볼루션은 실제로 합성곱을 완전히 되돌려 원본 텐서를 반환한다. 전치 합성곱은 이 절차와 거의 비슷해 원본과 형상이 같은 텐서를 반환할 뿐이다. 원본 텐서와 마지막 결과 텐서의 형상이 일치하지만, 그 값까지 동일하지는 않다. 경우에 따라 전치 합성곱을 보폭을 1 이하로 적용하는 합성곱(fractionally stride convolutions)라고도 한다. 실제로 입력 텐서의 팽창은 합성곱에 1 이하의 보폭을 사용하는 것과 동일하다고 볼 수 있다.

##### 언풀링
보폭이 적용된 합성곱이 CNN 아키텍처에 종종 사용된다면 이미지의 공간 차원을 축소하기 위해 가장 보편적으로 사용되는 연산은 평균-풀링과 최대-풀링이다. 따라서 제일러와 퍼거스도 최대-풀링을 유사-반전하기 위해 최대-언풀링 연산(종종 언풀링으로만 부른다)을 제안했다. 이들은 deconvnet이라고 하는 네트워크 내에서 이 연산을 사용해 'convnet'(즉, CNN)을 특징을 디코딩하고 시각화했다. ILSVRC 2013에서 우승한 뒤 이들의 솔루션을 설명하는 논문(Visualizing and Understanding Convolutional Networks)에서 최대-풀링이 역으로 뒤집을 수 없지만(즉 수학적으로 최대-풀링에서 폐기한 비최댓값들을 모두 복원할 수 없지만) 최소한 공간 샘플링 관점에서 그에 가깝게 연산을 정의할 수 있다고 설명했다.

이 유사-역연산을 구현하기 위해 먼저 각 최대-풀링 계층을 수정해 결과 텐서를 따라 풀링 마스크를 출력한다. 다시 말해서 이 마스크는 선택된 최댓값의 원래 위치를 가리킨다. 최대-언풀링 연산은 풀링된 텐서(이 텐서는 그 사이에 다른 형상 보존 연산을 거쳤을 수도 있다)와 풀링 마스크를 취한다. 이 연산은 풀링 마스크를 사용해 입력 값을 풀링되기 전의 형상으로 업스케일링된 텐서에 뿌린다. 풀링 계층처럼 언풀링 연산은 고정된/훈련할 수 없는 연산이다.

##### 업샘플링과 크기 조정
마찬가지로 평균-언풀링 연산은 평균-풀링 연산을 반전시키기 위해 개발됐다. 평균-풀링 연산은 k * k개의 요소를 갖는 풀링 영역을 위해 그 요소 값의 평균을 내 단일 값으로 만든다. 따라서 평균-언풀링 계층은 텐서의 각 값을 취해 이를 k * k 영역으로 복제한다. 평균-언풀링 연산이 최대-언풀링 연산보다 더 자주 사용되지만, 더 일반적으로는 업샘플링(Upsampling)으로 알려져 있다. 예를 들어 이 연산은 tf.keras.layers.UpSampling2D()를 통해 인스턴스화될 수 있다. 이 메서드 자체로는 최근접 이웃 보간법(이름이 뜻하는 대로)을 사용해 이미지 크기를 조정하기 위해 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR 인수로 호출될 때는 tf.image.resize()의 래퍼 함수일 뿐이다. 마지막으로 경우에 따라 훈련시킬 매개변수를 추가하지 않고 특징 맵을 업스케일링 하기 위해, 예를 들어 interpolation="bilinear"(기본값인 "nearest" 대신) 인수를 사용해 tf.keras.layers.UpSampling2D()를 인스턴스화함으로써 양선형 보간법도 사용된다. 이는 기본 속성인 method=tf.image.ResizeMethod.BILINEAR를 사용해 tf.image.resize()를 호출하는 것과 동일하다.

디코더 아키텍처에서 최근접 이웃 방식이나 양선형 방식 뒤에는 일반적으로 보폭이 s=1이고 패딩 옵션이 "SAME"(새 형상을 보존하기 위해)인 합성곱 계층이 따른다. 사전에 업스케일링과 합성곱 연산의 조합은 인코더를 구성하는 합성곱과 풀링 계층을 반전시킴으로써 디코더가 자신만의 특징을 학습해 타깃 시그널을 더 잘 복원할 수 있게 한다.

:bulb: 아우구스투스 오데나(Augustus Odena) 같은 일부 연구원들은 특히 이미지 해상도 개선과 같은 작업에서 전치 합성곱보다 이 연산을 주로 사용한다. 사실 전치 합성곱을 사용하면 바둑판 아티팩트가 생겨서(커널 크기가 보폭의 배수가 아닐 떄 특징이 서로 겹쳐서) 출력 품질에 영향을 주는 경향이 있다(Deconvolution and Checkerboard artifacts).

##### 팽창된/아트루스 합성곱
이 연산은 제공된 특징 맵을 업샘플링하는 용도로 쓰이지 않는다. 대신 이 연산은 데이터의 공간 차원을 더 희생시키지 않고 합성곱의 수용 영역을 인위적으로 증가시키기 위해 제안된다. 이를 달성하기 위해 상당히 다른 방식이긴 하지만 여기에도 팽창(Dilation) 연산이 적용된다.

사실 팽창된 합성곱(Dilated Convolution)은 추가적으로 커널에 적용될 팽창률을 정의하는 초매개변수가 있다는 점을 제외하면 표준 합성곱이랑 비슷하다. 

:bulb: 이 계층을 프랑스어로 '구멍이 난'이라는 뜻인 atrous에서 따온 아트루스 합성곱이라고도 한다. 실제로 커널 팽창률이 수용 영역을 증가시키지만, 수용 영역에 구멍을 만들어 증가시키기 때문이다.

이러한 속성을 갖춘 이 연산은 현대 인코더-디코더에서 한 도메인의 이미지를 다른 도메인으로 매핑하기 위해 자주 사용된다. 텐서플로와 케라스에서 팽창된 합성곱을 인스턴스화하려면 tf.layers.conv2d()와 tf.keras.layers.Conv2D()에서 dilation_rate 매개변수에 기본 값인 1 이상의 값을 제공하기만 하면 된다.

특징 맵의 공간성을 보존하거나 증가시키기 위해 개발된 이 다양한 연산들 때문에 픽셀 단위의 밀집된 예측과 데이터 생성을 위한 다양한 CNN 아키텍처가 만들어졌다.

#### 대표적인 아키텍처 - FCN과 U-Net
2015년에 출시된 FCN과 U-Net 모델은 여전히 인기가 많고 일반적으로 더 복잡한 시스템(의미론적 분할, 도메인 적응 등)을 위한 구성 요소로 사용된다.

##### FCN(Fully Convolutional Networks)
FCN(Fully Convolutional Networks)은 VGG-16 아키텍처에 기반해 마지막 밀집 계층을 1 * 1 합성곱 계층으로 대체해 구성된다. 여기에서 언급하지 않은 것은 이 네트워크가 일반적으로 업샘플링 블록을 사용해 확장되어 인코더-디코더로 사용된다는 점이다. 캘리포니아 대학교 버클리의 조나단 롱(Jonathan Long), 에반 셸하머(Evan Shelhamer), 트레버 대럴(Trevor Darrell)이 제안한 FCN 아키텍처는 앞에서 고안한 개념을 완벽하게 보여준다.

- 특징 추출을 위한 CNN을 효율적인 인코더로 사용할 수 있는 방법
- 그 다음 방금 소개한 연산을 사용해 CNN의 특징 맵을 효율적으로 업샘플링하고 디코딩하는 방법

사실 조나단 롱 팀은 사전 훈련된 VGG-16을 특징 추출기로 재사용할 것을 제안했다. VGG-16은 5개의 합성곱 블록을 사용해 이미지를 효율적으로 특징 맵으로 변환하지만 매 블록 다음에 공간 차원을 반으로 나눈다. 마지막 블록의 특징 맵을 디코딩하기 위해(예를 들어 의미론적 마스크로) 분류를 위해 사용되던 완전 연결 계층을 합성곱 계층으로 대체한다. 그런 다음 최종 계층으로 데이터를 입력 형상으로 업샘플링하기 위한 전치 합성곱을 적용한다(즉, VGG를 통해 공간 차원이 32로 나눠졌기 때문에 보폭을 s=32로 설정).

그렇지만 롱은 FCN-32s라는 이 아키텍처가 너무 조악한 결과를 낸다는 사실을 곧 깨달았다. 논문에서 설명했듯이(Fully Convolutional Networks for Semantic Segmentation), 최종 계층의 보폭이 크면 세부 사항의 크기를 제한한다. 마지막 VGG 블록의 특징에 풍부한 컨텍스트 정보를 포함하고 있더라도 공간을 정의하는 정보 대부분은 유실됐다. 그래서 저자는 마지막 블록의 특징 맵을 이전 블록의 더 큰 특징 맵과 결합하는 방법을 고안했다.

그에 따라 FCN-16s에서는 FCN-32s의 마지막 계층이 보폭 s=2밖에 되지 않는 전치 계층으로 대체돼 결과 텐서가 네 번째 블록의 특징 맵과 동일한 차원을 갖게 된다. 스킵 연결(skop connection)을 사용해 두 텐서에서 나온 특징이 서로 합쳐진다(요소-단위 덧셈). 마지막으로 이 결과를 s=16인 또 다른 전치 합성곱을 사용해 입력 형상으로 크기를 되돌린다. FCN-8s에서는 동일한 절차가 반복되는데, 대신 결과 텐서가 세 번째 블록의 특징과 같은 차원을 갖게 되며 마지막 전치 합성곱에서 보폭은 s=8을 사용한다. FCN-32s와 FCN-16s가 단 하나의 스킵 연결이 있거나 그마저도 없는 단순하고 가벼운 아키텍처임을 강조한다.

FCN-8s는 전치 학습을 사용하고 다양한 크기의 특징 맵을 결합함으로써 세부적인 부분까지 자세히 드러나는 이미지를 출력할 수 있다. 게다가 FCN의 본질 때문에 다양한 크기의 이미지를 인코딩/디코딩하는 데 FCN-8s를 적용할 수 있다. 성능이 뛰어나고 다재다능한 FCN-8s는 여러 다른 아키텍처에도 영감을 주면서 여전히 수많은 애플리케이션에서 보편적으로 사용된다.

##### U-Net
FCN으로부터 영감을 받은 솔루션 중 최초로 고안됐을 뿐만 아니라 가장 인기 있는 아키텍처는 U-Net일 것이다(올라프 로네베르거[Olaf Ronneberger], 필립 피셔[Philipp Fischer], 토머스 브록스[Thomas Brox]가 같은 해 Springer에 게재한 U-Net: Convolutinal Networks for biomedical image segmentation 논문에서 제안).

의미론적 분할(의학 촬영에 적용)을 위해 개발된 이 모델은 FCN과 여러 속성을 공유한다. 또한 특징 깊이를 증가시키면서 공간 차원은 축소시키는 다중블록 축소 인코더와 이미지 해상도를 복원하는 확장 디코더로 구성된다. 게다가 FCN처럼 스킵 연결은 인코딩 블록을 그에 대응하는 디코딩 블록과 연결한다. 따라서 디코딩 블록은 이전 블록으로부터 컨텍스트 정보를 제공받고 인코딩 경로에서 위치 정보를 제공 받는다.

또한 U-Net은 두 가지 주요 이유에서 FCN과 다르다. FCN-8s와 달리 U-Net은 전통적인 U자 형태의 인코더-디코더 구조(여기에서 이름을 따왔다)로 돌아가 대칭형이다. 또한 스킵 연결에서 나온 특징 맵은 덧셈 대신 연결(채널 축을 따라)을 통해 결합된다.

원래 디코딩 블록에는 업스케일링을 위해 s=2인 전치 합성곱 계층을 사용하지만, 그 대신 최근접 이웃 스케일링을 사용해 구현하는 경우도 일반적이다. U-Net은 그 인기를 감안하면 다양한 변형을 알고 있으며 여전히 수 많은 아키텍처에 영감을 준다(예를 들어 U-Net 블록을 잔차 블록으로 대체하거나 내부 블록과 외부 블록 사이의 연결 밀도를 높이는 등).

#### 중간 예정 - 이미지 해상도 개선

##### FCN 구현
케라스로 구현한 VGG와 함수형 API를 재사용하면 최소한의 노력으로 FCN-8s 모델을 생성할 수 있다.

##### 이미지 업스케일링에 적용
해상도 개선 작업을 위해 네트워크를 훈련시키는 간단한 요령으로는 이미지를 모델에 공급하기 전, 일부 전통적인 업스케일링 기법(양선형 보간법 같은)을 사용해 타깃 차원으로 척도를 맞추는 것이다. 이런 방식으로 네트워크는 업샘플링 아티팩트를 제거하고 손실된 세부 사항을 복원하는 노이즈 제거 오토인코더로 훈련될 수 있다.

```Python
x_noisy = bilinear_upscale(bilinear_downscale(x_train) # pseudo code
fcn_8s.fit(x_noisy, x_train)
```

## 의미론적 분할 이해하기
의미론적 분할(Semantic Segmentation)은 이미지를 의미 있는 부분으로 분할하는 작업을 지칭하는 좀 더 포괄적인 용어다. 객체 분할과 인스턴스 분할을 모두 아우른다. 이미지 분류와 객체 탐지와는 달리, 분할 작업에는 밀도 높은 픽셀 단위의 예측을 반환하는, 즉 입력 이미지의 각 픽셀마다 레이블을 할당하는 기법이 필요하다.

### 인코더-디코더를 사용한 객체 분할
이 장의 첫 번째 부분에서 봤듯이 인코딩-디코딩 네트워크는 한 도메인의 데이터 샘플을 다른 도메인으로 매핑(예를 들어 노이즈가 있는 데이터를 노이즈가 없는 데이터로, 컬러를 깊이로)하기 위해 훈련된다. 객체 분할을 컬러 도메인의 이미지를 클래스 도메인으로 매핑하는 것으로 보면 마찬가지 연산으로 볼 수 있다. 사진의 각 픽셀 값과 컨텍스트가 주어졌을 때 각 픽셀에 타깃 클래스 중 하나를 할당해 동이란 높이와 너비를 갖는 레이블 맵(label map)을 반환해야 한다.

이미지를 취해 레이블 맵을 반환하도록 인코더-디코더를 가르칠 때는 몇 가지 고려할 사항이 있다.

#### 개요
U-Net 같은 네트워크가 객체 분할에 어떻게 사용되는지와 어떻게 그 출력을 더 처리해서 정제된 레이블 맵을 생성하는지 보여준다.

##### 레이블 맵으로 디코딩하기
인코더-디코더를 각 픽셀 값이 클래스(예를 들어, 1은 '개', 2는 '고양이' 등)를 나타내는 레이블 맵을 바로 출력하도록 구성하면 그 결과의 품질이 현저히 떨어진다. 분류기를 사용할 때와 마찬가지로 범주형 값을 출력하는 더 나은 방식이 필요하다.

앞에서 이미지를 N개의 범주로 분류하기 위해 최종 계층에서 클래스당 예측된 점수를 나타내는 N개의 로짓을 출력하는 네트워크를 구성하는 방법을 배웠다. 또한 소프트맥스 연산을 사용해 이 점수를 확률로 전환하는 방법과 그 중 가장 높은 점수를 선정해(예를 들어, argmax를 사용해) 가장 확률 높은 클래스를 반환하는 방법도 배웠다. 의므론적 분할에서도 동일한 기법을 사용할 수 있는데, 대신 이미지 단위가 아니라 픽셀 단위로 적용된다. 전체 이미지 단위로 각 이미지마다 클래스당 점수를 포함하는 N개 로짓의 컬럼 벡터를 출력하는 대신 이 네트워크는 픽셀 단위의 점수를 담고 있는 H * W * N 텐서를 반환하도록 구성된다.

이 장에서 보여준 아키텍처의 경우 이러한 출력 텐서를 얻으려면 모델을 구성할 때 D_0 = N으로, 즉 출력 채널의 개수를 클래스 개수와 동일하게 설정하기만 하면 된다. 그런 다음 분류기처럼 훈련시키면 된다. 소프트맥스 값과 실제 값을 원-핫 인코딩으로 나타낸 레이블 맵과 비교하기 위해 교차-엔트로피 손실(cross-entropy loss)을 사용한다(비교되는 텐서에는 분류를 위한 계산에서는 영향을 미치지 않는 차원들이 더 있다). 또한 이와 유사하게 H * W * N 예측은 채널 축을 따라 가장 높은 값을 갖는 인덱스를 선택함으로써(즉, 채널 축에서 argmax를 적용) 픽셀별 레이블로 변환될 수 있다. 예를 들어 앞에서 보여준 FCN-8s 코드는 다음처럼 객체 분할을 위해 모델을 훈련시키도록 조정될 수 있다.

```Python
inputs = Input(shape=(224, 224, 3))
out_ch = num_classes = 19 # object segmentation of Cityscapes dataset
# [...] FCN-8s 아키텍처 구성
outputs = Conv2DTranspose(filters=out_ch, kernel_size=16, strides=8, padding='same', activation=None)(m2)
seg_fcn = Model(inputs, outputs)
seg_fcn.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
# [...] 네트워크 훈련, 그런 다음 이를 사용해 레이블 맵을 예측:
label_map = np.argmax(seg_fcn.predict(image), axis=-1)
```

##### 분할 손실과 지표를 사용해 훈련하기
교차-엔트로피는 성긴 분류와 조밀한 분류 모두를 위해 모델을 훈련시킬 때 기본 손실로 사용되지만, 조밀한 분류의 경우 몇 가지 예방책이 필요하다.

이미지 단위 분류와 픽셀 단위 분류 작업에서 보통 클래스 불균형 문제가 발생한다. 990개의 'cat' 사진과 10개의 'dog' 사진으로 구성된 데이터셋에서 모델을 훈련시킨다고 가정하자. 항상 고양이를 출력하도록 학습한 모델은 99%의 정확도를 보이지만 실제로 현장에서 유용하지는 않을 것이다. 이미지 분류에서는 모든 클래스가 동일한 비율로 등장하도록 사진을 추가 또는 삭제함으로써 이 문제를 피할 수 있다. 그렇지만 픽셀 단위 분류에서는 이 문제가 해결하기가 까다롭다. 어떤 클래스는 모든 이미지에 나타날 수 있지만 소수의 픽셀에만 적용되는 반면 다른 클래스는 이미지 대부분을 차지할 수 있다(자율 주행 자동차 애플리케이션에서 'traffic sign'과 'road' 클래스처럼). 이 데이터셋은 그러한 불균형을 보완하기 위해 편집될 수 없다.

대신 분할 모델이 더 큰 클래스로 편향되는 것을 방지하기 위해 그 손실 함수가 조정돼야 한다. 예를 들어 각 클래스의 교차 엔트로피 손실에 대한 기여도를 측정하는 것이 일반적이다. 자율 주행 자동차를 위한 의미론적 분류를 다루는 노트북과 훈련 이미지에서 등장하는 클래스가 작을수록 손실에 대한 가중치가 높아진다. 이 방식으로 작은 클래스를 무시하기 시작하면 네트워크에 페널티가 크게 부과된다.

가중치 맵은 일반적으로 실제 레이블 맵으로부터 계산된다. 각 픽셀에 적용된 가중치는 클래스에 따라 설정될 뿐만 아니라 다른 요소와 상대적인 픽셀 위치 등에 따라 설정된다.

또 다른 해법으로는 교차 엔트로피 자체를 클래스 비율에 영향받지 않는 다른 비용 함수로 교체하는 것이 있다. 결국 교차 엔트로피는 정확도 함수를 대리하는 함수이며 만족스러운 수준으로 미분 가능하기 때문에 사용된다. 그렇지만 실제로 이 함수가 모델의 실제 목표인 영역에 관계없이 다른 클래스를 올바르게 분할하고 있는지를 표현하지 않는다. 따라서 수많은 연구원들이 이 목표를 제대로 달성했는지 명확하게 평가하기 위해 의미론적 분할에 특화된 여러 손실 함수와 지표를 제안해 왔다.

IoT(Intersection-over-Union)는 이러한 일반적인 지표 중 하나다. 또 다른 지표로는 쇠렌센-다이스 계수(Sorensen-Dice coefficient, 대체로 간단하게 다이스 계수라고 부름)가 있다. IoU처럼 이 계수는 두 집합이 얼마나 잘 겹치는지 측정한다.

Dice(A, B) = 2 * |A & B| / (|A| + |B|)

여기에서 |A|와 |B|는 각 집합의 크기를 나타내며, |A & B|는 두 집합이 공통으로 갖는 요소의 개수를 나타낸다(두 집합의 교집합의 크기). IoU와 다이스 계수는 몇 가지 속성을 공유하며 실제로 하나가 다른 하나를 계산하는 데 도움이 될 수 있다.

IoU(A, B) = Dice(A, B) / (2 - Dice(A, B)); Dice(A, B) = 2 * IoU(A, B) / (1 + IoU(A, B))

의미론적 분할에서 'Dice' 계수는 각 클래스에 대해 예측된 마스크가 실제 마스크와 얼마나 잘 겹치는지 측정하기 위해 사용된다. 한 클래스에 대해 분자는 정확하게 분류된 픽셀 개수를 나타내고 분모는 예측된 마스크와 실제 마스크 모두에서 이 클래스에 속한 전체 픽셀 개수를 나타낸다. 따라서 지표로서 'Dice' 계수는 한 클래스가 이미지에서 취하는 상대적 픽셀 수에 따라 달라지지 않는다. 다중 클래스 분할 작업에서 과학자는 일반적으로 각 클래스에 대한 'Dice' 계수를 계산한 다음(예측된 마스크와 실제 마스크 각 쌍을 비교해서) 그 결과의 평균을 낸다.

이 공식에서 'Dice' 계수는 0과 1 사이의 값으로 정의됨을 볼 수 있다. 그 값은 A와 B가 전혀 겹치지 않으면 0이고 완전히 겹치면 1이다. 따라서 네트워크가 최소화해야 할 손실 함수로 다이스 계수를 사용하려면 이 점수를 뒤집어야 한다. 대체로 N개의 클래스에 적용되는 의미론적 분할의 경우 '다이스' 손실은 일반적으로 다음과 같이 정의된다.

L<sub>Dice</sub>(y, y<sup>true</sup> = 1 - 1/N * sum(Dice(y[k], y<sup>true</sup>[k] 여기에서 Dice(a, b) = (e + 2 * sum(a dot b)[i][j]) / (e + sum(a[i][j] + sum(b[i][j])

이 공식을 조금 명확히 하자. a와 b가 원-핫 인코딩된 두 개의 텐서라면 다이스 분자(즉, 둘의 교집합)는 둘 사이에 요소 단위 곱셈을 적용한 다음 결과 텐서의 값을 모두 더함으로써 근사될 수 있다. 분모는 a와 b의 요소를 모두 더함으로써 얻을 수 있다. 마지막으로 작은 값(예를 들어 1e-6보다 작은) 을 분모에 더함으로써 텐서가 아무 것도 포함하지 않았을 때 0으로 나누는 경우를 피하고 마찬가지로 분자에 더함으로써 결과를 매끄럽게 한다.

:bulb: 실제 값을 담은 원-핫 인코딩된 텐서와 달리, 실제로 예측 텐서는 이진 값을 포함하지 않는다. 예측 텐서는 0부터 1사이 연속 범위에 포함되는 소프트맥스 확률로 구성된다. 따라서 이 손실을 종종 소프트 다이스(soft Dice)라고 부르기도 한다.

텐서플로에서 이 손실은 다음처럼 구현될 수 있다.

```Python
def dice_loss(labels, logits, num_classes, eps=1e-6, spatial_axes=[1, 2]):
  # 로짓을 확률로, 실제 값을 원-핫 인코딩된 텐서로 변환:
  pred_proba = tf.nn.softmax(logits, axis=-1)
  gt_onehot = tf.one_hot(labels, num_classes, type=tf.float32)
  # 다이스 계수의 분자와 분모 계산:
  num_perclass = 2 * tf.reduce_sum(pred_proba * gt_onehot, axis=spatial_axes)
  den_perclass = tf.reduce_sum(pred_proba + gt_onehot, axis=spatial_axes)
  # 배치와 클래스에 대한 평균과 다이스 계수 계산:
  dice = tf.reduce_mean((num_perclass + eps) / (den_perclass + eps))
  return 1 - dice
```

다이스 계수와 IoU 모두 분할 작업에 있어 중요한 도구이며 관련 주피터 노트북에서 이 도구들이 얼마나 유용한지 볼 수 있다.

##### 조건부 랜덤 필드로 후처리
모든 픽셀에 레이블을 올바르게 지정하는 일은 복잡한 작업이며, 윤곽이 뚜렷하지 않고 부정확한 영역이 작은 예측 레이블 맵을 얻는 것이 일반적이다. 다행스럽게도 결과를 후처리해 명백한 단점들을 바로잡는 기법들이 있다. 이 기법들 중에 조건부 랜덤 필드(conditional random fields, CRFs) 기법이 전체 효율성 측면에서 가장 유명하다.

그 이론적 배경을 여기서는 다루지는 않겠지만, CRF는 원본 이미지로 돌아가 각 픽셀의 컨텍스트를 고려함으로써 픽셀 단위의 예측을 개선할 수 있다. 두 개의 이웃한 픽셀 사이의 색 변화가 작으면(즉, 색이 갑자기 변하지 않으면) 이 둘은 동일한 클래스에 속할 가능성이 있다. 공간과 색 기반의 모델과 예측기가 제공하는 확률 맵(여기서는 CNN에서 출력한 소프트맥스 텐서)을 고려해 CRF 기법은 시각적 윤곽선 측면에서 더 나은 정교화된 레이블 맵을 반환한다.

예를 들어 필립 크라헨불(Philipp Krahenbuhl)과 블라들렌 콜툰(Vladlen Koltun)이 제안한 (Efficient inference in fully connected CRFs with gaussian edge potentials) 가우스 테두리 후보를 사용하는 고밀도 CRF의 파이썬 래퍼 함수로 루카스 바이엘(Lucas Beyer)이 개발한 pydensecrf와 같은 여러 가지 기존 구현물을 사용할 수 있다. 이 장의 마지막 주피터 노트북에서 이 프레임워크를 사용하는 방법을 보여준다.

#### 고급 예제 - 자율 주행 자동차를 위한 이미지 분할

##### 작업 설명
자율 주행 자동차는 사람처럼 환경을 이해하고 주변을 둘러싼 요소들을 인지해야 한다. 전방 카메라로부터 유입되는 동영상 이미지에 의미론적 분할을 적용함으로써 시스템은 다른 자동차가 주변에 있는지 또는 보행자나 자전거가 길을 건너고 있지는 않은지를 알 수 있고 차선과 교통 신호를 지킬 수 있다.

##### 대표적인 솔루선
이 작업을 해결하기 위해 FCN와 U-Net 모델을 이 절에서 보여주는 몇 가지 비결을 사용해 훈련시킨디ㅏ. 손실을 계산할 때 각 클래스를 올바르게 평가하는 방법과 레이블 맵을 사후 처리하는 방법 등을 보여준다.

### 더 까다로운 인스턴스 분할
객체 분할을 위해 훈련된 모델을 사용하면 '소프트맥스' 출력은 각 픽셀에 대해 N개 클래스 중 하나에 속할 확률을 나타낸다. 그렇지만 이 출력이 두 픽셀 혹은 픽셀 블롭(blob)이 한 클래스의 동일한 인스턴스에 속하는지 여부를 나타내지는 않는다.

이제부터는 앞에서 이미 해결한 두 가지 관련 작업인 객체 분할과 객체 탐지를 위한 솔루션을 확장함으러써 인스턴스 분할을 달성하는 두 가지 방식을 설명한다.

#### 객체 분할에서 인스턴스 분할까지
인스턴스 마스크를 얻기 위해 몇 가지 도구를 설명하겠다. U-Net의 저자는 인코더-디코더를 조정해서 그 출력을 인스턴스 세분화에 사용할 수 있도록 하는 아이디어를 대중화했다. 이 아이디어는 의료 애플리케이션용 인스턴스 분할을 촉진하기 위한 후원 대회인 캐글 2018 데이터 사이언스 볼(Kaggle's Data Science Bowl)에서 우승한 알렉산더 부슬라에프(Alexander Buslaev), 빅터 두르노프(Victor Durnov), 셀림 세페르베코프(Selim Seferbekov)에 의해 더 발전됐다.

### 더 까다로운 인스턴스 분할

#### 객체 분할에서 인스턴스 분할까지

##### 경계를 고려하기

##### 사후 처리를 통해 인스턴스 마스크로 변환

#### 객체 탐지부터 인스턴스 분할까지 - Mask R-CNN

##### 의미론적 분할을 경계 상자에 적용하기

##### Faster R-CNN으로부터 인스턴스 분할 모델 구성하기

:bulb: 카이밍 히는 ResNet과 Faster R-CNN의 주 저자이다.

## 요약
픽셀 단위의 정확도를 필요로 하는 애플리케이션을 위한 다양한 패러다임을 다뤘다. 인코더-디코더와 일부 특수 아키텍처를 소개했고 이를 이미지 노이즈 제거에서 의미론적 분할에 이르기까지 여러 작업에 적용해 봤다. 또한 인스턴스 분할처럼 진화된 문제를 해결하기 위해 다양한 솔루션을 결합하는 방법도 보여줬다.

점점 더 복잡한 작업을 해결할수록 새로운 도전 과제들이 등장한다. 예를 들어 의미론적 분할의 경우 모델을 훈련시키기 위해 이미지에 정확하게 주석을 다는 일은 시간이 많이 걸린다. 따라서 활용할 수 있는 데이터셋은 부족하기 때문에 과적합을 피하기 위해 특별한 측정 기준이 필요하다. 게다가 훈련 이미지와 해당 이미지의 실제 정보가 더 무겁기 때문에 효율적인 훈련을 위해서는 잘 설계된 데이터 파이프라인이 필요하다.

## 질문

## 참고 문헌
