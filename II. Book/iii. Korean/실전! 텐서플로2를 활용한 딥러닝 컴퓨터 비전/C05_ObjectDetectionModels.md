# 05 객체 탐지 모델
최신 기법 중 가장 일반적으로 사용되는 두 모델인 YOLO(You Only Look Once)와 R-CNN(Regions with Convolutional Neural Networks) 아키텍처를 설명한다.

- 객체 탐지 기법의 역사
- 주요 객체 탐지 방법
- YOLO 아키텍처를 사용해 빠른 객체 탐지 구현하기
- Faster R-CNN 아키텍처를 사용해 객체 탐지 성능 개선하기
- 텐서플로 객체 탐지 API로 Faster R-CNN 사용하기

## 기술 요구사항

## 객체 탐지 소개

### 배경
객체 탐지(Object Detection) 또는 객체 위치 특정(Object Localization)이라고도 하는 이 프로세스는 한 이미지에서 객체와 그 경계 상자를 탐지한다. 경계 상자(Bounding Box)는 이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형이다.

객체 탐지 알고리즘에서는 일반적으로 이미지를 입력으로 받고 경계 상자와 객체 클래스 리스트를 출력한다. 모델은 각 경계 상자에 대해 그에 대응하는 예측 클래스와 해당 클래스의 신뢰도(Confidence)를 출력한다. 

#### 애플리케이션

#### 약력
객체 탐지는 전통적인 컴퓨터 비전 기법인 이미지 설명자(Image Descriptors)를 기반으로 한다. 예를 들어 자전거 같은 객체를 탐지하려면 이 객체가 포함된 몇 장의 사진으로 시작한다. 자전거에 해당하는 설명자는 이미지로부터 추출된다. 그 설명자는 이미지로부터 추출된다. 그 설명자는 자전거의 특정 부분을 나타낸다. 알고리즘이 이 객체를 찾을 때 목표 이미지에서 다시 설명자를 찾으려고 할 것이다.

이미지에서 자전거를 찾기 위해 가장 일반적으로 사용되는 기법은 플로팅 윈도우(Floating Window)다. 이미지의 작은 직사각형 영역이 차례로 검사된다. 가장 일치하는 설명자를 가진 부분이 해당 객체를 포함하는 것으로 간주된다.

이 기법은 이미지를 회정하거나 색이 바뀌더라도 성능에 영향을 주지 않고 훈련 데이터가 많이 필요하지 않으며 대부분의 객체에 동작한다. 하지만 정확도는 만족스럽지 않다.

성능은 알고리즘이 다음 항목에서 얼마나 우수한지를 나타낸다.

- 경계 상자 정밀도(Bounding Box Precision): 정확한 경계 상자(너무 크지도, 너무 작지도 않은)를 제공하는가?
- 재현율(Recall): 모든 객체를 찾았는가? (어떤 객체도 놓치지 않았는가?)
- 클래스 정밀도(Class Precision): 객체마다 정확한 클래스를 출력했는가? (고양이를 개로 착각하지 않았는가?)

성능 개선은 모델이 결과를 계산하는 속도가 빨라졌음(특정 입력 이미지에 대해 특정 컴퓨팅 파워로)을 뜻하기도 한다. 초기 모델은 객체를 탐지하는 데 상당한 시간(몇 초보다 오랜 시간)이 걸렸지만, 지금은 실시간으로 사용될 수 있다. 컴퓨터 비전에서 실시간이란 일반적으로 1초에 5개 이상 탐지하는 속도를 뜻한다.

### 모델 성능 평가

#### 정밀도와 재현율
일반적으로 정밀도와 재현율은 객체 탐지 모델 평가에 사용되지 않지만, 다른 지표를 계샇나는 기본 지표 역할을 한다.

정밀도와 재현율을 측정하기 위해서는 먼저 각 이미지에 대해 다음을 계산해야 한다.

- 참긍정 수: 참긍정(True Positive, TP)은 얼마나 많은 예측이 동일 클래스의 실제 상자와 일치하는지 측정한다.
- 거짓긍정 수: 거짓긍정(False Positive, FP)은 얼마나 많은 예측이 동일 클래스의 실제 상자와 일치하지 않는지 측정한다.
- 거짓부성 수: 거짓부정(False Negative, FN)은 얼마나 많은 실제 분류 값이 그와 일치하는 예측을 갖지 못하는지 측정한다.

정밀도(Precision)와 재현율(Recall)은 다음과 같이 정의된다.

precision = TP / (TP + FP)

recall = TP / (TP + FN)

예측이 실제 분류와 정확히 일치하면 거짓긍정이나 거짓부정이 없을 것이다. 따라서 정밀도와 재현율에서 만점은 1이다. 대체로 모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정이 많아져 정밀도가 낮아진다. 반대로 모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주한다면 거짓부정이 많아져서 재현율이 낮아질 것이다.

#### 정밀도-재현율 곡선
정밀도-재현율 곡선(Precision-Recall Curve)은 수많은 머신러닝 모델에서 사용된다. 일반적인 개념은 신뢰도 임곗값마다 모델의 정밀도와 재현율을 시각화하는 것이다. 모델은 모든 경계 상자와 함께 모델이 예측의 정확성을 얼마나 확신하는지를 0과 1 사이의 숫자로 나타내는 신뢰도(Confidence)를 출력한다.

신뢰도가 낮은 예측은 유지할 필요가 없으므로 일반적으로 특정 임곗값 T 이하의 예측은 제거한다. 예를 들어 T=0.4라면 이 숫자 이하의 신뢰도를 갖는 예측은 고려 대상에서 제외한다.

임곗값을 바꾸면 정밀도와 재현율도 달라진다.

- T가 1에 가까우면: 정밀도는 높지만 재현율은 낮다. 객체를 많이 걸러내기 때문에 놓치는 객체가 많아져 재현율이 낮아진다. 신뢰도가 높은 예측만 유지하므로 거짓긍정 수가 많지 않아 정밀도는 높아진다.
- T가 0에 가까우면: 정밀도는 낮지만 재현율은 높다. 대부분의 예측을 유지하므로 거짓부정이 없어져 재현율이 높아진다. 모델이 자신의 예측에 대한 확신이 낮기 때문에 거짓긍정이 많아져 정밀도가 낮아진다.

:bulb: 임곗값은 정확도(Accuracy)와 재현율 사이의 트레이드오프를 고려해 정해야 한다. 모델이 보행자를 탐지하고 있다면 때때로 마땅한 이유 없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 한다. 모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 한다.

#### AP와 mAP
정밀도-재현 곡선이 모델에 대해 많은 것을 말해줄 수 있지만, 대체로 하나의 숫자로 이해하는 것이 더 편리하다. AP(Average Precision, 평균 정밀도)는 곡선의 아래 영역에 해당한다. 이 영역은 항상 1 * 1 정사각형으로 구성되므로 AP는 항상 0에서 1 사이의 값을 갖는다.

AP는 단일 클래스에 대한 모델 성능 정보를 제공한다. 전역 점수를 얻기 위해 mAP(Mean Average Precision)를 사용한다. 이는 각 클래스에 대한 AP의 평균에 해당한다. 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고 다시 그 숫자의 평균을 구한다.

:bulb: mAP는 최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes(일반적으로 Pascal VOC라고도 함)와 Common Objects in Context(일반적으로 COCO라고 함)에서 사용된다. COCO 데이터셋이 더 크고 많은 클래스를 포함하고 있으므로 여기에서 얻는 점수는 Pascal VOC보다 더 낮다.

#### AP 임곗값
TP과 FP가 실제 상자와 일치하거나 일치하지 않는 예측 개수에 의해 정의된다고 설명했다. 그렇지만 예측과 실제가 언제 일치하는지 어떻게 결정할까? 일반적으로 두 집합(여기에서는 상자로 표현되는 픽셀 집합)이 얼마나 겹치는지 측정하는 자카드 지표(Jaccard Index)를 메트릭으로 사용한다. IoT(Intersection over Union)라고도 알려진 이 지표는 다음과 같이 정의된다.

IoU(A, B) = | intersection(A, B) | / | union(A, B) | = | intersection(A, B) | / (|A| +|B| - | intersection(A, B)|)

|A|와 |B|는 각 집합의 카디널리티(Cardinality)로, 각 집합이 포함한 요소의 개수를 말한다. intersection(A, B)는 두 집합의 교집합으로 분자 |intersection(A, B)|는 두 집합이 공통으로 갖고 있는 요소 개수를 나타낸다. 마찬가지로 union(A, B)는 두 집합의 합집합으로 분모 |union(A, B)|는 두 집합이 함께 가지고 있는 전체 요소 개수를 나타낸다.

교집합은 두 집합/상자가 얼마나 겹치는지를 보기에는 좋은 지표지만, 이 값은 절대적인 수치일 뿐 상대적이지 않다. 따라서 두 개의 큰 상자가 아마 두 개의 작은 상자보다 훨씬 많은 픽셀이 겹칠 것이다. 그렇기 때문에 이 비율을 사용한다. 이 비율은 항상 0(두 상자가 전혀 겹치지 않는 경우)과 1(두 상자가 완전히 겹치는 경우) 사이의 값을 갖는다.

AP를 계산할 때 IoU가 특정 임곗값을 넘으면 두 상자가 겹친다고 말한다. 일반적으로 이 임곗값은 0.5로 정한다.

:bulb: Pascal VOC 대회에서도 0.5를 사용하며 mAP@0.5라고 부른다. COCO 대회에서는 약간 다른 지표로 mAP@[0.5:0.95]를 사용한다. 이는 mAP@0.5, mAP@0.95를 계산해 평균을 구한다는 뜻이다. IoU를 평균하면 모델의 위치 측정 성능이 좋아진다.

## 빠른 객체 탐지 알고리즘 - YOLO
최신 버전은 YOLOv3은 크기가 256x256인 이미지에 대해 최신 GPU에서 초당 170프레임(170FPS, frames per second)의 속도로 실행될 수 있다.

### YOLO 소개
2015년에 최초로 공개된 YOLO는 속도와 정확도 측면 모두에서 거의 모든 객체 탐지 아키텍처를 능가했다. 그 이후로 이 아키텍처는 몇 차례 개선됐다.

- You Only Look Once: Unified, real-time object detection (2015), Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhad
- YOLO9000: Better, Faster, Stronger (2016), Joseph Redmon, Ali Farhadi
- YOLOv3: An Incremental Improvement (2018), Joseph Redmon, Ali Farhadi

:bulb: YOLO 논문의 1저자는 다크넷(Darknet)이라는 딥러닝 프레임워크를 관리한다. 이 프레임워크는 YOLO의 공식 구현을 제공하고 있어 논문의 결과를 재현하는 데 사용될 수 있다. 이 코드는 C++로 구현됐으며 텐서플로를 기반으로 하지 않는다.

#### YOLO의 강점과 한계
YOLO는 속도가 빠른 것으로 유명하다. 하지만 Faster R-CNN이 정확도 측면에서 YOLO를 능가했다. 게다가 YOLO는 객체를 탐지하는 방식 때문에 작은 크기의 물건을 탐지하는 데 어려움을 겪는다. 예를 들어 새 무리에서 개별 새를 탐지하는 데 문제가 있을 수 있다. 대부분의 딥러닝 모델과 마찬가지로 훈련 세트에서 너무 많이 벗어난 객체(모양이나 가로/세로 비율이 이례적인 경우)를 적절히 탐지하는 일에도 어려움을 겪는다. 그럼에도 불구하고 이 아키텍처는 꾸준히 진화하고 있으며 이러한 문제들을 해결하고 있다.

#### YOLO의 주요 개념
YOLO의 핵심 아이디어는 객체 탐지를 단일 회귀 문제로 다시 구성하는 것이다. 슬라이딩 윈도우나 다른 복잡한 기법을 사용하는 대신 입력을 w * h로 나눈다. 그리드의 각 부분에 대해 B개의 경계 상자를 정의한다. 그런 다음 각 경계 상자에 대해 다음을 예측하기만 하면 된다.

- 상자의 중심
- 상자의 너비와 높이
- 이 상자가 객체를 포함하고 있을 확률
- 앞서 말한 객체의 클래스

이 모든 예측은 숫자이므로 객체 탐지 문제를 회귀 문제로 변환했다. 실제로 YOLO에서 사용하는 개념은 이보다 좀 더 복잡하다.

### YOLO로 추론하기
추론(Inference)은 이미지 입력을 받아 결과를 계산하는 절차다. 훈련은 모델의 가중치를 학습하는 절차다. 모델을 처음부터 구현할 때는 모델이 훈련되기 전에는 추론을 사용할 수 없다. 그렇지만 설명을 단순화하기 위해 추론부터 알아본다.

#### YOLO 백본
대부분의 이미지 탐지 모델처럼 YOLO는 백본 모델(Backbone Model)을 기반으로 한다. 이 모델의 역할은 마지막 계층에서 사용될 의미 있는 특징을 이미지로부터 추출하는 것이다. 이 때문에 백본 모델을 특징 추출기(Feature Extractor)라고도 부른다. 특징 추출기로 어떤 아키텍처도 사용할 수 있지만 YOLO 논문에서는 맞춤 아키텍처를 사용했다. 최종 모델의 성능은 특징 추출기 아키텍처로 무엇을 사용했는지에 따라 크게 달라진다.

백본의 마지막 계층은 크기가 w * h * D인 특징 볼륨을 출력하는데, 여기에서 w * h는 그리드의 크기이고 D는 특징 볼륨의 깊이다. 예를 들어, VGG-16의 경우 D=512다.

그리드 크기인 w * h는 다음 2 요인에 따라 달라진다.

- 전체 특징 추출기의 보폭: VGG-16의 보폭은 16으로, 출력된 특징 볼륨이 입력 이미지보다 16배 작다는 뜻이다.
- 입력 이미지 크기: 특징 볼륨 크기는 이미지 크기에 비례하므로 입력 크기가 작을수록 그리드 크기가 작아진다.

YOLO의 마지막 계층은 입력으로 특징 볼륨을 받는다. 이 마지막 계층은 크기가 1 * 1인 합성곱 필터로 구성된다. 크기가 1 * 1인 합성곱 계층은 특징 볼륨의 공간 구조에 영향을 주지 않고 깊이를 바꾸는 데 사용될 수 있다.

#### YOLO의 계층 출력
YOLO의 마지막 출력은 w * h * M 행렬로, 여기에서 w * h는 그리드 크기이며 M은 공식 B * (C + 5)에 해당한다. B와 C는 각각 다음과 같이 정의할 수 있다.

- B는 그리드 셀당 경계 상자 개수이다.
- C는 클래스 개수다.

클래스 개수에 5를 더했다는 점에 유념하자. 이는 경계 상자마다 (C + 5)개의 숫자를 예측해야 하기 때문이다.

- t_x와 t_y는 경계 상자의 중심 좌표를 계산하기 위해 사용된다.
- t_w와 t_h는 경계 상자의 너비와 높이를 계산하기 위해 사용된다.
- c는 객체가 경계 상자 안에 있다고 확신하는 신뢰도다.
- p1, p2, ..., p_C는 경계 상자 클래스 1, 2, ..., C의 객체를 포함할 확률이다.

#### 앵커 박스 소개
t_x, t_y, t_w, t_h가 경계 상자의 좌표를 계산하는 데 사용된다고 설명했다. 왜 네트워크 좌표(x, y, w, h)를 직접 출력할 것을 요청하지 않을까? 실제로 YOLOv1에서는 그 방식을 사용했다. 유감스럽게도 이 방법은 객체 크기가 다양하기 때문에 수많은 오차가 발생한다.

사실 훈련 데이터셋의 객체 대부분이 크면 네트워크 w와 h가 매우 크다고 예측할 것이다. 그리고 작은 객체에서 훈련된 모델을 사용할 때 이 네트워크는 대체로 실패할 것이다. 이 문제를 해결하기 위해 YOLOv2에서는 앵커 박스(Anchor Box)를 도입했다.

앵커 박스(사전 정의된 상자, Prior Box)는 네트워크를 훈련시키기 전에 결정되는 일련의 경계 상자 크기다. 예를 들어 보행자를 탐지하기 위해 신경망을 훈련시킬 때 크고 가는 앵커 박스를 선택한다. 경계 상자에 따른 보정은 신경망에 의해 계산된다.

앵커 박스의 집합은 일반적으로 작으며 실제로 3~25 사이의 다양한 크기를 갖는다. 그러한 상자가 모든 객체와 정확하게 일치할 수는 없으므로 네트워크는 가장 근접한 앵커 박스를 개선하는 데 사용된다. 예제에서는 이미지의 보행자를 가장 근접한 앵커 박스와 맞추고 신경망을 사용해 앵커 박스의 높이를 보정한다. 이것이 t_x, t_y, t_w, t_h가 필요한 이유로, 앵커 박스 보정에 해당한다.

최초로 논문에서 앵커 박스를 소개했을 때는 수작업으로 앵커 박스를 선택했다. 일반적으로 9개의 상자 크기가 사용됐다.

- 3개의 정사각형(작은 크기, 중간 크기, 큰 크기)
- 3개의 가로로 긴 직사각형(작은 크기, 중간 크기, 큰 크기)
- 3개의 세로로 긴 직사각형(작은 크기, 중간 크기, 큰 크기)

하지만 YOLOv2 논문에서 저자들은 앵커 박스의 크기가 데이터셋마다 다르다는 점을 인정했다. 따라서 모델을 훈련시키기 전에 데이터를 분석해 앵커 박스의 크기를 선택할 것을 권고하고 있다. 예를 들어, 보행자를 탐지하기 위해서라면 세로로 긴 직사각형을 사용할 것이고, 사과를 탐지하는 데는 정사각형 앵커 박스가 사용될 것이다.

#### YOLO가 앵커 박스를 개선하는 방법
YOLOv2는 다음 공식을 사용해 최종 경계 상자 좌표를 각각 계산한다.

b_x = sigmoid(t_x) + c_x

b_y = sigmoid(t_y) + c_y

b_w = p_w * exp(t_w)

b_h = p_h * exp(t_h)

t_x, t_y, t_w, t_h는 마지막 계층의 출력이다. b_x, b_y, b_w, b_h는 각각 예측된 경계 상자의 위치와 크기다. p_w, p_h는 앵커 박스의 원래 크기를 나타낸다. c_x, c_y는 현재 그리드 셀의 좌표다(상단 왼쪽 상자는 (0, 0), 상단 오른쪽 상자는 (w-1, 0), 하단 왼쪽 상자는 (0, h-1) 등으로 한다). 

#### 상자를 사후 처리하기

#### NMS

#### YOLO 추론 요약

### YOLO 훈련시키기

#### YOLO 백본 훈련 방법

#### YOLO 손실

##### 경계 상자 손실

##### 객체 신뢰도 손실

##### 분류 손실

##### 전체 YOLO 손실

#### 훈련 기법

## Faster R-CNN - 강력한 객체 탐지 모델

### Faster R-CNN의 일반 아키텍처

#### 1단계 - 영역 제안

#### 2단계 - 분류

##### Fast R-CNN 아키텍처

##### RoI 풀링

### Faster R-CNN 훈련

#### RPN 훈련시키기

#### RPN 손실

#### Fast R-CNN 손실

#### 훈련 계획

### 텐서플로 객체 탐지 API

#### 사전 훈련된 모델 사용하기

#### 맞춤 데이터셋에서 훈련하기

## 요약

## 질문

## 참고 문헌
