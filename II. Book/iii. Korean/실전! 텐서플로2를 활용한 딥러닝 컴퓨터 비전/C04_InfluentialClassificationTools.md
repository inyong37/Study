# 04 유력한 분류 도구
- VGG, Inception, ResNet 같은 아키텍처가 컴퓨터 비전 분야에 가져온 것
- 이 솔루션들을 분류 작업을 위해 재구현하거나 직접 재활용할 방법
- 전이학습의 정의와 훈련된 네트워크를 다른 용도에 맞게 효율적으로 바꾸는 방법

## 기술 요구사항

## 고급 CNN 아키텍처의 이해

### VGG, 표준 CNN 아키텍처

#### VGG 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - CNN 아키텍처 표준화

##### 규모가 큰 합성곱을 여러 작은 합성곱으로 대체
- 매개변수 개수를 줄인다: 실제로 11 * 11 합성곱 계층에 N개의 필터를 적용한다는 것은 커널만을 위해 훈련시켜야 할 값이 11 * 11 * D * N=121DN개라는 것을 뜻한다. 반면 5개의 3 * 3 합성곱에는 커널을 위한 가중치가 총 1 * (3 * 3 * D * N) + 4 (3 * 3 * N * N)=9DN + 36N^2개가 있다. N < 3.6D 이기만 하면 매개변수 개수가 더 작다는 뜻이다. 예를 들어 N = 2D이면 매개변수 개수는 242D^2에서 153D^2개로 떨어진다. 이렇게 되면 네트워크를 최적화하기 쉬워지고 훨씬 가벼워진다.
- 비선형을 증가시킨다: 합성곱 계층 개수가 커지면, 그리고 각 합성곱 계층 다음에 ReLU 같은 '비선형' 활성화 함수가 오면 네트워크가 복잡한 특징을 학습할 수 있는 능력이 증대된다(즉, 더 많은 비선형 연산을 결합함으로써).

##### 특징 맵 깊이를 증가
직관을 기반으로 각 합성곱 블록에 대한 특징 맵의 깊이를 두 배로 늘렸다(첫 번째 합성곱 다음에 64에서 512로). 각 집합 다음에는 윈도우 크기가 2 * 2이고 보폭이 2인 최대-풀링 계층이 나오므로 깊이가 두 배로 늘고 공간 차원은 반으로 줄게 된다.

##### 척도 변경을 통한 데이터 보강

##### 완전 연결 계층을 합성곱 계층으로 대체
전통적인 VGG 아키텍처는 마지막에 여러 개의 오나전 연결 계층이 오지만(AlexNet처럼), 이 논문에서는 다른 방식을 제안한다. 여기서 제안한 아키텍처에서는 밀집 계층을 합성곱 계층으로 대체한다. 크기가 좀 더 큰 커널(7 * 7과 3 * 3)을 적용한 첫 번째 합성곱 세트는 특징 맵의 공간 크기를 1 * 1로 줄이고(그 전에 패딩을 적용하지 않았다면) 특징 맵의 깊이를 4096으로 늘린다. 마지막으로 1 * 1 합성곱 계층이 예측해야 할 클래스 개수만큼의 필터(즉, ImageNet의 경우 N=1000)와 함께 사용된다. 그 결과 얻게 된 1 * 1 * N 벡터는 softmax 함수로 정규화된 다음 평면화되어 최종 클래스 예층으로 출력된다(벡터의 각 값은 예측된 클래스 확률을 나타낸다).

밀집 계층을 두지 않는 이러한 네트워크를 완전 합성곱 계층(Fully Connected Network, FCN)이라고 한다. FCN은 사전에 이미지를 자르지 않고도 다양한 크기의 이미지에 적용될 수 있다.

:bulb: 흥미롭게도 ILSVRC에서 가장 높은 정확도를 얻기 위해 저자는 일반 합성곱 신경망과 FCN을 모두 훈련시켜 사용하고 두 결과의 평균을 구해 최종 예측을 얻었다. 이러한 기법을 모델 평균법(Model Averaging)이라고 하고 실제 운영 환경에서 자주 사용한다.

#### 텐서플로와 케라스로 구현하기

##### 텐서플로 모델

##### 케라스 모델

### GoogLeNet, Inception 모듈

#### GoogLeNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 규모가 큰 블록과 병목을 보편화

##### 인셉션 모듈로 다양한 세부 특징 잡아내기

##### 병목 계층으로 1 * 1 합성곱 계층을 사용

:bulb: 이 장에서는 ILSVRC 2014에 제출된 GoogLeNet을 설명한다. 일반적으로 사용하는 명칭은 인셉션 V1으로, 이 아키텍처는 그 이후로 저자들에 의해 개선됐다. 인셉션 V2와 인셉션 V3에는 5 * 5와 7 * 7 합성곱 계층을 그보다 작은 합성곱 계층으로 대체하고(VGG에서와 마찬가지로) 정보 손실을 줄이기 위해 병목 계층의 초매개변수를 개선하거나 'BatchNorm' 계층을 추가하는 등 몇 가지 개선사항이 포함돼 있다. 

##### 완전 연결 계층 대신 풀링 계층 사용

##### 중간 손실로 경사 소실 문제 해결하기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 Inception 모듈 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

### ResNet - 잔차 네트워크

#### ResNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 정보를 더 깊은 계층으로 전방 전달

##### 매핑 대신 잔차 함수 추정하기

##### '극단적으로 깊이' 들어가기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 잔차 블록 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

## :pencil: 전이학습 활용

### 개요

#### 정의

##### 인간으로부터 영감 얻기
기존에 보유한 지식을 기반으로 복잡한 작업에 숙달하거나 비슷한 행동에 이미 가지고 있는 기술을 바꿔 적용하는 능력은 인간 지능에서 가장 중요한 부분이다. 머신러닝 연구원들은 이 능력을 복제하는 꿈을 꾸고 있다.

##### 동기
CNN은 특정 특징을 추출해 해석하도록 훈련됐기 때문에 특징 분포가 바뀌면 그 성능은 떨어지게 된다. 따라서 네트워크를 새로운 작어베 적용하려면 어느 정도의 변환 작업이 필요하다.

그 해법에 대해 수십년간 연구됐다. 1998년 세바스찬 스런(Sebastian Thrun)과 로리엔 프랫(Lorien Pratt)은 이 주제에 기초한 널리 알려진 연구를 엮어 Learning to Learn이라는 책을 편집했다. 더 최근에는 이안 굿펠로(Ian Goodfellow)와 요슈아 벤지오(Yoshua Bengio), 애런 쿠빌(Aaron Courille)이 심층 학습(Deep Learning)에서 전이학습을 다음과 같이 정의했다. *어떤 설정(예를 들어, 분포 P_1)에서 학습된 내용이 다른 설정(분포 P_2라고 하자)에서 일반화를 개선하기 위해 활용되는 상황*

:bulb: 머신러닝에서 작업(task)는 주어진 입력(예를 들어, 스마트폰으로 찍은 사진)과 예상 출력(예를 들어, 특정 클래스 집합에 대한 예측 결과)에 의해 정의된다. 예를 들어, ImageNet에서 분류하거나 탐지하는 작업은 입력 이미지는 동일하지만 출력은 다르기 때문에 서로 다른 두 개의 작업으로 봐야 한다. 경우에 따라 알고리즘은 유사한 작업(예를 들어, 보행자 탐지)을 목표로 하지만 다른 데이터셋(예를 들어, 다양한 위치의 CCTV 이미지나 다양한 품질의 카메라로 찍은 이미지)를 사용하기도 한다. 따라서 이 기법은 다양한 도메인(즉, 데이터 분포)에서 훈련된다. 전이학습은 이미 갖춘 지식을 하나의 작업에서 다른 작업으로 또는 한 도메인에서 다른 도메인으로 적용하는 것을 목표로 한다. 이 중 한 도메인에서 다른 도메인으로 적용하는 형태의 전이학습을 도메인 적응(Domain Adaption)이라고 한다.

전이학습은 새로운 작업을 적절하게 학습하기에 충분한 데이터가 확보되지 않았을 때(즉, 분포를 추정하기에 이미지 샘플이 충분하지 않을 때) 매력적이다. 특히 지도 학습을 위해 레이블이 붙은 데이터셋을 수집하는 일은 불가능하지 않더라도 지루하고 따분한 작업이다.

:bulb: ImageNet(최근에는 COCO)은 수많은 카테고리에서 나온 주석이 달른 이미지를 수백만 개 포함하고 있어 특히 풍부한 데이터셋이다. 이 데이터셋에서 훈련된 CNN은 시각 인식 작업에서 상당한 전문 역량을 습득했다고 가정하므로 케라스과 텐서플로 허브에서도 이 데이터셋에서 이미 훈련된 표준 모델(인셉션, ResNet-50 등)을 제공한다. 지식전이할 모델을 찾는 사람들은 일반적으로 이 표준 모델을 사용한다.

##### CNN 지식 전이
인공 신경망이 사람의 뇌보다 나은 점 중 하나는 저장과 복제가 쉽다는 점이다. CNN이 갖고 있는 전문지식은 훈련 이후 매개변수가 취한 값, 즉 쉽게 복원되고 유사한 네트워크에 전이될 수 있는 값이 전부다.

CNN을 위한 전이학습은 주로 다른 작업을 위한 새로운 모델을 인스턴스화하기 위해 풍부한 데이터셋에서 훈련된 성능 좋은 네트워크의 아키텍처 전체 혹은 일부와 가중치를 재사용하는 것으로 구성된다. 새로운 모델은 이 조건에 따라 인스턴스화한 다음 '미세 조정'될 수 있다. 즉 새로운 작업/도메인에 대해 활용할 수 있는 데이터에서 더 훈련될 수 있다.

네트워크의 첫 번째 계층은 저차원 특징(선, 테두리, 색 변화 등)을 추출하는 경향이 있지만 마지막 합성곱 계층은 더 복잡한 개념(특정 형태나 패턴 같이)에 반응한다. 분류 작업을 위해 마지막 풀링 또는 완전 연결 계층에서 클래스를 예측하기 위해 이 고차원 특징 맵(병목 특징, Bottleneck Feature)을 처리한다.

이 일반적인 구성과 관련 관측을 통해 다양한 전이학습 전략이 도출됐다. 마지막 예측 계층을 제거한 사전 훈련된 CNN은 효율적으로 '특징을 추출'하는 용도로 사용되기 시작했다. 새로운 작업이 이 추출기가 훈련된 목적과 충분히 비슷한 경우 적절한 특징을 출력하기 위해 바로 사용될 수 있다(이 목적에 정확히 부합하는 '이미지 특정 벡터' 모델을 텐서플로 허브에서 찾아볼 수 있다). 그런 다음 이 특징은 작업과 관련된 예측을 출력하도록 훈련된 한두 개의 새로 추가된 밀집 계층에서 처리될 수 있다. 추출된 특징의 품질을 유지하기 위해 대체로 이 훈련 단계 동안 특징 추출기의 계층을 '고정'시킨다. 즉, 그 계층의 매개변수는 경사 하강이 일어나는 동안 업데이트되지 않는다. 이와 다르게 작업/도메인이 유사하지 않은 경우 특징 추출기의 마지막 계층 중 일부 또는 전체를 '미세 조정'한다. 즉, 이 계층들을 작업 데이터에서 새로운 예측 계층과 함께 훈련시킨다.

#### 활용 사례

##### 제한적 훈련 데이터로 유사한 작업 수행

##### 풍부한 훈련 데이터로 유사한 작업 수행

##### 풍부한 훈련 데이터로 유사하지 않은 작업 수행

##### 제한적 훈련 데이터로 유사하지 않은 작업 수행

### 텐서플로와 케라스로 전이학습 구현

#### 모델 수술

##### 계층 제거

##### 계층 이식

#### 선택적 훈련

##### 사전 훈련된 매개변수 복원하기

##### 계층 고정하기

## 요약

## 질문

## 
