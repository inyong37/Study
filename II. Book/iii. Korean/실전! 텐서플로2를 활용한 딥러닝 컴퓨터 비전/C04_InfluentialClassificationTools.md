# 04 유력한 분류 도구
- VGG, Inception, ResNet 같은 아키텍처가 컴퓨터 비전 분야에 가져온 것
- 이 솔루션들을 분류 작업을 위해 재구현하거나 직접 재활용할 방법
- 전이학습의 정의와 훈련된 네트워크를 다른 용도에 맞게 효율적으로 바꾸는 방법

## 기술 요구사항

## 고급 CNN 아키텍처의 이해

### VGG, 표준 CNN 아키텍처

#### VGG 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - CNN 아키텍처 표준화

##### 규모가 큰 합성곱을 여러 작은 합성곱으로 대체
- 매개변수 개수를 줄인다: 실제로 11 * 11 합성곱 계층에 N개의 필터를 적용한다는 것은 커널만을 위해 훈련시켜야 할 값이 11 * 11 * D * N=121DN개라는 것을 뜻한다. 반면 5개의 3 * 3 합성곱에는 커널을 위한 가중치가 총 1 * (3 * 3 * D * N) + 4 (3 * 3 * N * N)=9DN + 36N^2개가 있다. N < 3.6D 이기만 하면 매개변수 개수가 더 작다는 뜻이다. 예를 들어 N = 2D이면 매개변수 개수는 242D^2에서 153D^2개로 떨어진다. 이렇게 되면 네트워크를 최적화하기 쉬워지고 훨씬 가벼워진다.
- 비선형을 증가시킨다: 합성곱 계층 개수가 커지면, 그리고 각 합성곱 계층 다음에 ReLU 같은 '비선형' 활성화 함수가 오면 네트워크가 복잡한 특징을 학습할 수 있는 능력이 증대된다(즉, 더 많은 비선형 연산을 결합함으로써).

##### 특징 맵 깊이를 증가
직관을 기반으로 각 합성곱 블록에 대한 특징 맵의 깊이를 두 배로 늘렸다(첫 번째 합성곱 다음에 64에서 512로). 각 집합 다음에는 윈도우 크기가 2 * 2이고 보폭이 2인 최대-풀링 계층이 나오므로 깊이가 두 배로 늘고 공간 차원은 반으로 줄게 된다.

##### 척도 변경을 통한 데이터 보강

##### 완전 연결 계층을 합성곱 계층으로 대체
전통적인 VGG 아키텍처는 마지막에 여러 개의 오나전 연결 계층이 오지만(AlexNet처럼), 이 논문에서는 다른 방식을 제안한다. 여기서 제안한 아키텍처에서는 밀집 계층을 합성곱 계층으로 대체한다. 크기가 좀 더 큰 커널(7 * 7과 3 * 3)을 적용한 첫 번째 합성곱 세트는 특징 맵의 공간 크기를 1 * 1로 줄이고(그 전에 패딩을 적용하지 않았다면) 특징 맵의 깊이를 4096으로 늘린다. 마지막으로 1 * 1 합성곱 계층이 예측해야 할 클래스 개수만큼의 필터(즉, ImageNet의 경우 N=1000)와 함께 사용된다. 그 결과 얻게 된 1 * 1 * N 벡터는 softmax 함수로 정규화된 다음 평면화되어 최종 클래스 예층으로 출력된다(벡터의 각 값은 예측된 클래스 확률을 나타낸다).

밀집 계층을 두지 않는 이러한 네트워크를 완전 합성곱 계층(Fully Connected Network, FCN)이라고 한다. FCN은 사전에 이미지를 자르지 않고도 다양한 크기의 이미지에 적용될 수 있다.

:bulb: 흥미롭게도 ILSVRC에서 가장 높은 정확도를 얻기 위해 저자는 일반 합성곱 신경망과 FCN을 모두 훈련시켜 사용하고 두 결과의 평균을 구해 최종 예측을 얻었다. 이러한 기법을 모델 평균법(Model Averaging)이라고 하고 실제 운영 환경에서 자주 사용한다.

#### 텐서플로와 케라스로 구현하기

##### 텐서플로 모델

##### 케라스 모델

### GoogLeNet, Inception 모듈

#### GoogLeNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 규모가 큰 블록과 병목을 보편화

##### 인셉션 모듈로 다양한 세부 특징 잡아내기

##### 병목 계층으로 1 * 1 합성곱 계층을 사용

:bulb: 이 장에서는 ILSVRC 2014에 제출된 GoogLeNet을 설명한다. 일반적으로 사용하는 명칭은 인셉션 V1으로, 이 아키텍처는 그 이후로 저자들에 의해 개선됐다. 인셉션 V2와 인셉션 V3에는 5 * 5와 7 * 7 합성곱 계층을 그보다 작은 합성곱 계층으로 대체하고(VGG에서와 마찬가지로) 정보 손실을 줄이기 위해 병목 계층의 초매개변수를 개선하거나 'BatchNorm' 계층을 추가하는 등 몇 가지 개선사항이 포함돼 있다. 

##### 완전 연결 계층 대신 풀링 계층 사용

##### 중간 손실로 경사 소실 문제 해결하기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 Inception 모듈 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

### ResNet - 잔차 네트워크

#### ResNet 아키텍처 개요

##### 동기

##### 아키텍처

#### 기여 - 정보를 더 깊은 계층으로 전방 전달

##### 매핑 대신 잔차 함수 추정하기

##### '극단적으로 깊이' 들어가기

#### 텐서플로와 케라스로 구현하기

##### 케라스 함수형 API로 잔차 블록 구현하기

##### 텐서플로 모델과 텐서플로 허브

##### 케라스 모델

## :pencil: 전이학습 활용

### 개요

#### 정의

##### 인간으로부터 영감 얻기
기존에 보유한 지식을 기반으로 복잡한 작업에 숙달하거나 비슷한 행동에 이미 가지고 있는 기술을 바꿔 적용하는 능력은 인간 지능에서 가장 중요한 부분이다. 머신러닝 연구원들은 이 능력을 복제하는 꿈을 꾸고 있다.

##### 동기
CNN은 특정 특징을 추출해 해석하도록 훈련됐기 때문에 특징 분포가 바뀌면 그 성능은 떨어지게 된다. 따라서 네트워크를 새로운 작어베 적용하려면 어느 정도의 변환 작업이 필요하다.

##### CNN 지식 전이

#### 활용 사례

##### 제한적 훈련 데이터로 유사한 작업 수행

##### 풍부한 훈련 데이터로 유사한 작업 수행

##### 풍부한 훈련 데이터로 유사하지 않은 작업 수행

##### 제한적 훈련 데이터로 유사하지 않은 작업 수행

### 텐서플로와 케라스로 전이학습 구현

#### 모델 수술

##### 계층 제거

##### 계층 이식

#### 선택적 훈련

##### 사전 훈련된 매개변수 복원하기

##### 계층 고정하기

## 요약

## 질문

## 
