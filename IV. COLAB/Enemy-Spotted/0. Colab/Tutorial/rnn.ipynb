{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKSd91vkb5Y7aVWCFxhF4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inyong37/Study/blob/master/IV.%2520COLAB/Enemy-Spotted/0.%2520Colab%2520Tutorial/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Neural Network\n",
        "\n",
        "Date: 2024-10-21-Mon.\n",
        "\n",
        "RNN, LSTM, GRU는 순환 신경망(Recurrent Neural Network, RNN) 계열의 모델로, 시퀸스 데이터를 다루는 데 매우 효과적이다. 이 모델들은 텍스트 데이터(예: 영화 리뷰 분류)나 시간 순서에 따른 데이터를 분석하는 데 많이 사용된다. RNN은 순차적으로 데이터가 입력되면서 이전 상태를 메모리처럼 사용하지만, LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)는 더 복잡한 구조로 긴 시퀀스에서 장기 종속성을 잘 처리하는 데 적합하다.\n",
        "\n",
        "1. RNN: 시퀀스 데이터의 각 요소를 순차적으로 처리하며, 이전 시간 단계의 상태를 현재 단계로 전달한다. 하지만 장기 종속성(Long-Term Dependency)을 잘 기억하지 못하는 문제가 있다.\n",
        "2. LSTM: RNN의 단점을 보완한 모델로, 장기 종속성을 더 잘 기억할 수 있도록 게이트 구조(입력 게이트, 출력 게이트, 망각 게이트)를 사용한다.\n",
        "3. GRU: LSTM의 간소화된 버전으로, 비슷한 성능을 유지하면서 계산 비용을 줄일 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "eZLLLMoQ45X2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZinZAK1y43hn",
        "outputId": "cd431f03-c572-4ab9-ebd4-ba0027bea586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (25000, 500)\n",
            "Test data shape: (25000, 500)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
        "\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000 # Vocabulary size\n",
        "maxlen = 500 # Maximum number of words in a sequence\n",
        "\n",
        "# Load the dataset (train/test splits)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Test data shape: {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple RNN model\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(max_features, 32)) # Embedding layer for word representations\n",
        "rnn_model.add(SimpleRNN(32)) # Simple RNN layer\n",
        "rnn_model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_rnn = rnn_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URhbO7XK6kRS",
        "outputId": "417257f1-07d6-4104-eaf5-69289b754ad6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 60ms/step - accuracy: 0.6673 - loss: 0.5957 - val_accuracy: 0.8338 - val_loss: 0.3995\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - accuracy: 0.8690 - loss: 0.3230 - val_accuracy: 0.8384 - val_loss: 0.3811\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - accuracy: 0.9070 - loss: 0.2426 - val_accuracy: 0.8120 - val_loss: 0.4396\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.9511 - loss: 0.1509 - val_accuracy: 0.7788 - val_loss: 0.6764\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 55ms/step - accuracy: 0.9641 - loss: 0.1071 - val_accuracy: 0.8076 - val_loss: 0.5548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(max_features, 32)) # Embedding layer\n",
        "lstm_model.add(LSTM(32)) # LSTM layer\n",
        "lstm_model.add(Dense(1, activation='sigmoid')) # Output layer\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_lstm = lstm_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YJGdW047JEK",
        "outputId": "203f80d5-7419-48de-bd3f-65c9ce5b7c21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 24ms/step - accuracy: 0.6514 - loss: 0.6229 - val_accuracy: 0.8464 - val_loss: 0.3640\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.8762 - loss: 0.3176 - val_accuracy: 0.8536 - val_loss: 0.3395\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9167 - loss: 0.2263 - val_accuracy: 0.8666 - val_loss: 0.3187\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9435 - loss: 0.1625 - val_accuracy: 0.8734 - val_loss: 0.3228\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9564 - loss: 0.1310 - val_accuracy: 0.8754 - val_loss: 0.3612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a GRU model\n",
        "gru_model = Sequential()\n",
        "gru_model.add(Embedding(max_features, 32)) # Embedding layer\n",
        "gru_model.add(GRU(32)) # GRU layer\n",
        "gru_model.add(Dense(1, activation='sigmoid')) # Output layer\n",
        "\n",
        "# Compile the model\n",
        "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_gru = gru_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fm4cQc37iPM",
        "outputId": "abb0d0f7-fb08-4c05-e517-e7c166c6de2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6547 - loss: 0.5910 - val_accuracy: 0.8560 - val_loss: 0.3434\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8942 - loss: 0.2685 - val_accuracy: 0.8692 - val_loss: 0.3188\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9208 - loss: 0.2081 - val_accuracy: 0.8718 - val_loss: 0.3283\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9444 - loss: 0.1569 - val_accuracy: 0.8358 - val_loss: 0.3867\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9482 - loss: 0.1411 - val_accuracy: 0.8732 - val_loss: 0.3458\n"
          ]
        }
      ]
    }
  ]
}
