# [Long Language Model](https://aws.amazon.com/what-is/large-language-model/?nc1=h_ls)

Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.

Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. It is through this process that transformers learn to understand basic grammar, languages, and knowledge.

Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.

Transformer neural network architecture allows the use of very large models, often with hundreds of billions of parameters. Such large-scale models can ingest massive amounts of data, often from the internet, but also from sources such as the Common Crawl, which comprises more than 50 billion web pages, and Wikipedia, which has approximately 57 million pages.

## [LLaMa](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)

---

### Reference
- What is LLM (Large Language Model) AWS, https://aws.amazon.com/what-is/large-language-model/?nc1=h_ls, 2025-01-07-Tue.
- Introducing LLaMa Meta, https://ai.meta.com/blog/large-language-model-llama-meta-ai/, 2025-01-07-Tue.
- LLM Project Blog KR, https://aidalab.tistory.com/239, 2025-01-06-Mon.
